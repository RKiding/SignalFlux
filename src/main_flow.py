import os
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Union
from loguru import logger
from dotenv import load_dotenv

from utils.database_manager import DatabaseManager
from utils.llm.factory import get_model
from utils.llm.router import router
from utils.search_tools import SearchTools
from utils.json_utils import extract_json
from agents import TrendAgent, FinAgent, ReportAgent, IntentAgent
from schema.models import InvestmentSignal, InvestmentReport
from agno.agent import Agent
from utils.md_to_html import save_report_as_html
from prompts.trend_agent import get_news_filter_instructions
from utils.checkpointing import CheckpointManager, resolve_latest_run_id
from utils.logging_setup import setup_file_logging, make_run_id

class SignalFluxWorkflow:
    """
    SignalFlux ä¸»å·¥ä½œæµ
    
    æµç¨‹:
    1. TrendAgent -> æ‰«æçƒ­ç‚¹
    2. FinAgent -> æ·±åº¦åˆ†æ
    3. ReportAgent -> ç”Ÿæˆç ”æŠ¥
    """
    
    def __init__(self, db_path: str = "data/signal_flux.db", isq_template_id: str = "default_isq_v1"):
        load_dotenv()
        self.isq_template_id = isq_template_id
        # åˆå§‹åŒ–æ•°æ®åº“
        self.db = DatabaseManager(db_path)
        
        # ä½¿ç”¨ ModelRouter è·å–ä¸åŒç”¨é€”çš„æ¨¡å‹
        self.reasoning_model = router.get_reasoning_model()
        self.tool_model = router.get_tool_model()
        
        # åˆå§‹åŒ– Agents
        # TrendAgent ä½¿ç”¨åŒæ¨¡å‹ï¼šç­›é€‰ä½¿ç”¨ reasoning_modelï¼Œé‡‡é›†ä½¿ç”¨ tool_model
        self.trend_agent = TrendAgent(self.db, self.reasoning_model, tool_model=self.tool_model, sentiment_mode="bert")
        # FinAgent ä½¿ç”¨åŒæ¨¡å‹ï¼šåˆ†æä½¿ç”¨ reasoning_modelï¼Œæ£€ç´¢ä½¿ç”¨ tool_modelï¼ŒISQ æ¨¡æ¿å¯é…ç½®
        self.fin_agent = FinAgent(self.db, self.reasoning_model, tool_model=self.tool_model, isq_template_id=self.isq_template_id)
        # ReportAgent æ”¯æŒåŒæ¨¡å‹ï¼šå†™ä½œä½¿ç”¨ reasoning_modelï¼Œæ£€ç´¢ä½¿ç”¨ tool_model
        self.report_agent = ReportAgent(self.db, self.reasoning_model, tool_model=self.tool_model)
        # æ„å›¾åˆ†æä¸»è¦æ˜¯æ–‡æœ¬ç†è§£ï¼Œä½¿ç”¨æ¨ç†æ¨¡å‹
        self.intent_agent = IntentAgent(self.reasoning_model)
        self.search_tools = SearchTools(self.db)
        
        # ç”¨äºç­›é€‰çš„è½»é‡ Agentï¼ˆä¸å¸¦å·¥å…·ï¼‰ï¼Œä½¿ç”¨æ¨ç†æ¨¡å‹
        self.filter_agent = Agent(model=self.reasoning_model, markdown=True, debug_mode=True)
        
        logger.info(f"ğŸš€ SignalFlux Workflow initialized with Dual-Model Routing (ISQ Template: {self.isq_template_id})")
    
    def _llm_filter_signals(self, news_list: List[Dict], depth: Union[int, str], query: Optional[str] = None) -> List[Dict]:
        """ä½¿ç”¨ LLM æ™ºèƒ½ç­›é€‰é«˜ä»·å€¼ä¿¡å·
        
        ä½¿ç”¨ FilterResult schema å¿«é€Ÿåˆ¤æ–­æ˜¯å¦æœ‰æœ‰æ•ˆä¿¡å·ï¼Œé¿å…å¤„ç†æ— æ•ˆå†…å®¹
        """
        if isinstance(depth, int) and len(news_list) <= depth and not query:
            return news_list
        
        # æ„å»ºæ–°é—»åˆ—è¡¨æ–‡æœ¬
        news_text = "\n".join([
            f"[ID: {n.get('id', i)}] {n['title']} (æƒ…ç»ª: {n.get('sentiment_score', 'N/A')})"
            for i, n in enumerate(news_list)
        ])
        
        # ç”Ÿæˆç­›é€‰ prompt (å¸¦ query)
        filter_instruction = get_news_filter_instructions(len(news_list), depth, query)
        self.filter_agent.instructions = [filter_instruction]
        
        try:
            response = self.filter_agent.run(f"è¯·ç­›é€‰ä»¥ä¸‹æ–°é—»:\n{news_text}")
            content = response.content
            
            # æå– JSON
            result = extract_json(content)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆä¿¡å·ï¼ˆå‡å°‘ token æ¶ˆè€—ï¼‰
            if result and not result.get("has_valid_signals", True):
                logger.warning(f"âš ï¸ No valid signals found: {result.get('reason', 'Unknown')}")
                return []
            if not result:
                logger.warning(f"Failed to parse LLM filter response: {content}")
                return news_list
            
            selected_ids = result.get("selected_ids", [])
            themes = result.get("themes", [])
            
            logger.info(f"ğŸ¯ LLM ç­›é€‰ç»“æœ: {len(selected_ids)} æ¡, {len(themes)} ä¸ªä¸»é¢˜")
            
            # æ ¹æ® ID ç­›é€‰æ–°é—»
            id_set = set(str(sid) for sid in selected_ids)
            filtered = [n for n in news_list if str(n.get('id', '')) in id_set]
            
            # åŠ¨æ€é€»è¾‘ï¼š
            # 1. åªæœ‰åœ¨ LLM æœªé€‰å‡ºä»»ä½•å†…å®¹ä¸”éç‰¹å®šæŸ¥è¯¢æ—¶ï¼Œæ‰å›é€€åˆ°é»˜è®¤å‰å‡ æ¡
            if not filtered and not query:
                 logger.warning("âš ï¸ LLM selected 0 items, falling back to top items")
                 return news_list
            
            # 2. å¦‚æœæœ‰ queryï¼Œå®Œå…¨ä¿¡ä»» LLM çš„é€‰æ‹©ï¼ˆæ•°é‡å¯èƒ½å°‘äºæˆ–å¤šäº depthï¼‰
            if query:
                return filtered
            
            # 3. å¦‚æœæ˜¯é€šç”¨æ‰«æï¼Œé™åˆ¶æœ€å¤§è¿”å›æ•°é‡
            return filtered
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLM ç­›é€‰å¤±è´¥: {e}, å›é€€åˆ°å…¨éƒ¨è¿”å›")
            return news_list

    # å¯ç”¨çš„æ–°é—»æºï¼ˆæŒ‰ç±»åˆ«ï¼‰
    FINANCIAL_SOURCES = ["cls", "wallstreetcn", "xueqiu", "eastmoney", "yicai"]
    SOCIAL_SOURCES = ["weibo", "zhihu", "baidu", "toutiao", "douyin"]
    TECH_SOURCES = ["36kr", "ithome", "v2ex", "juejin", "hackernews"]
    ALL_SOURCES = FINANCIAL_SOURCES + SOCIAL_SOURCES + TECH_SOURCES
    
    def run(
        self,
        sources: List[str] = None,
        wide: int = 10,
        depth: Union[int, str] = 'auto',
        query: Optional[str] = None,
        run_id: Optional[str] = None,
        resume: bool = False,
        checkpoint_dir: str = "reports/checkpoints",
    ) -> Optional[str]:
        """æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            sources: æ–°é—»æ¥æºåˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["all"]
            wide:  æ–°é—»æŠ“å–å¹¿åº¦ï¼ˆæ¯ä¸ªæºæŠ“å–çš„æ•°é‡ï¼‰
            depth: ç”ŸæˆæŠ¥å‘Šçš„æ·±åº¦ï¼Œè‹¥ä¸º 'auto'ï¼Œåˆ™ç”± LLM æ€»ç»“åˆ¤æ–­ï¼Œè‹¥ä¸ºæ•´æ•°åˆ™é™åˆ¶æœ€åç”Ÿæˆçš„ä¿¡å·æ•°é‡
            query:  ç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚ "é¦™æ¸¯ç«ç¾"ã€"Aè‚¡ç§‘æŠ€æ¿å—"
            
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œæˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        # Resolve run_id and checkpoint manager
        if resume and not run_id:
            run_id = resolve_latest_run_id(checkpoint_dir)
            if not run_id:
                logger.warning("âš ï¸ resume requested but no checkpoint runs found; starting fresh")
        run_id = run_id or datetime.now().strftime('%Y%m%d_%H%M%S')
        ckpt = CheckpointManager(base_dir=checkpoint_dir, run_id=run_id)
        os.makedirs(ckpt.run_dir, exist_ok=True)

        ckpt.save_json(
            "state.json",
            {
                "run_id": run_id,
                "resume": bool(resume),
                "started_at": datetime.now().isoformat(),
                "params": {"sources": sources, "wide": wide, "depth": depth, "query": query},
                "status": "running",
            },
        )

        if sources is None:
            sources = ["all"]

        # Fast resume paths
        if resume and ckpt.exists("report.md"):
            logger.info(f"â™»ï¸ Resuming: found final report checkpoint for run_id={run_id}")
            report_md = ckpt.load_text("report.md")
            if report_md:
                report_dir = "reports"
                os.makedirs(report_dir, exist_ok=True)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M')
                md_filename = f"{report_dir}/daily_report_{timestamp}.md"
                with open(md_filename, "w", encoding="utf-8") as f:
                    f.write(report_md)
                html_filename = save_report_as_html(md_filename)
                ckpt.save_json("state.json", {"run_id": run_id, "status": "completed", "resumed_from": "report.md", "finished_at": datetime.now().isoformat()})
                return html_filename or md_filename
            
        logger.info("--- Step 1: Trend Discovery ---")
        
        # 0. æ„å›¾åˆ†æ (å¦‚æœå­˜åœ¨ query)
        intent_info = ""
        if query:
            logger.info(f"ğŸ§  Analyzing intent for: {query}")
            intent_info = self.intent_agent.run(query)
            ckpt.save_json("intent.json", intent_info)
        
        # 1. è§£æ sources å‚æ•°
        if "all" in sources:
            actual_sources = self.ALL_SOURCES.copy()
        elif "financial" in sources:
            actual_sources = self.FINANCIAL_SOURCES.copy()
        elif "social" in sources:
            actual_sources = self.SOCIAL_SOURCES.copy()
        elif "tech" in sources:
            actual_sources = self.TECH_SOURCES.copy()
        else:
            actual_sources = sources
        
        logger.info(f"ğŸ“¡ Attempting to fetch from {len(actual_sources)} sources...")
        
        # 2. è·å–çƒ­ç‚¹
        successful_sources = []
        for source in actual_sources:
            try:
                # ä½¿ç”¨ wide æ§åˆ¶æŠ“å–æ•°é‡
                result = self.trend_agent.news_toolkit.fetch_hot_news(source, count=wide)
                if result and len(result) > 0:
                    successful_sources.append(source)
                else:
                    logger.warning(f"âš ï¸ Source '{source}' returned no data, skipping")
            except Exception as e:
                logger.warning(f"âš ï¸ Source '{source}' failed: {e}, skipping")
        
        logger.info(f"âœ… Successfully fetched from {len(successful_sources)}/{len(actual_sources)} sources")
        ckpt.save_json(
            "trend_sources.json",
            {
                "actual_sources": actual_sources,
                "successful_sources": successful_sources,
                "wide": wide,
            },
        )
            
        # --- NEW: Active Search based on Intent ---
        search_signals = []
        if query and isinstance(intent_info, dict):
            search_queries = intent_info.get("search_queries", [query])
            is_specific = intent_info.get("is_specific_event", False)
            
            # å¦‚æœæ˜¯ç‰¹å®šäº‹ä»¶ï¼Œæˆ–è€…ç”¨æˆ·æ˜ç¡®æé—®ï¼Œæˆ‘ä»¬åº”è¯¥ä¸»åŠ¨æœç´¢
            if is_specific or len(search_queries) > 0:
                logger.info(f"ğŸ” Executing active search for queries: {search_queries}")
                for q in search_queries[:2]: # é™åˆ¶æŸ¥è¯¢æ•°ï¼Œé¿å…å¤ªæ…¢
                    # Consider using 'baidu' for Chinese queries if 'ddg' is unstable
                    # enrich=True is default, so we get full content
                    results = self.search_tools.search_list(q, engine="baidu", max_results=5, enrich=True)
                    for r in results:
                        # è½¬æ¢ä¸ºæ ‡å‡†ä¿¡å·æ ¼å¼ (search_tools now returns standard keys including id, rank, etc)
                        search_signals.append({
                            "title": r.get('title'),
                            "url": r.get('url'),
                            "source": r.get('source', 'Search'), # keeping original source name
                            "content": r.get('content'),
                            "publish_time": r.get('publish_time') or datetime.now(), 
                            "sentiment_score": r.get('sentiment_score', 0), 
                            "id": r.get('id') or f"search_{hash(r.get('url'))}"
                        })
                logger.info(f"ğŸ” Found {len(search_signals)} signals via search")
                ckpt.save_json("search_signals.json", {"query": query, "items": search_signals})

        # 2. æ‰¹é‡æ›´æ–°æƒ…ç»ªåˆ†æ•° (ä¿ç•™ï¼Œç”¨äºå¯è§†åŒ–)
        logger.info("Calculating sentiment scores...")
        self.trend_agent.sentiment_toolkit.batch_update_sentiment(limit=50)
        
        # 3. ä»æ•°æ®åº“è¯»å–æ–°é—» + åˆå¹¶æœç´¢ç»“æœ
        db_news = self.db.get_daily_news(limit=50)
        
        # åˆå¹¶åˆ—è¡¨ (Search signals preferred if query exists)
        raw_news = search_signals + db_news if search_signals else db_news
        
        if not raw_news:
            logger.warning("No news found in database.")
            return

        ckpt.save_json(
            "raw_news_meta.json",
            {
                "db_news_count": len(db_news) if db_news else 0,
                "search_signals_count": len(search_signals),
                "raw_news_count": len(raw_news),
            },
        )
        
        # 4. æ™ºèƒ½ç­›é€‰ï¼ˆLLM æˆ–ä¼ ç»Ÿæ–¹å¼ï¼‰
        # å¦‚æœæœ‰ queryï¼Œå³ä½¿æ•°é‡å°‘ä¹Ÿå»ºè®®èµ° LLM ç­›é€‰ä»¥åŒ¹é…ç›¸å…³æ€§
        if depth == 'auto' or query:
            logger.info(f"ğŸ¤– Using LLM to filter signals (Query: {query if query else 'Default'})...")
            high_value_signals = self._llm_filter_signals(raw_news, depth, query)
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šæŒ‰æƒ…ç»ªåˆ†æ•°æ’åº
            if isinstance(depth, int) and depth > 0:
                high_value_signals = sorted(
                    raw_news, 
                    key=lambda x: abs(x.get("sentiment_score") or 0), 
                    reverse=True
                )[:depth]
            else:
                high_value_signals = raw_news

        # Store a light checkpoint to resume analysis without rerunning filter
        try:
            hv_meta = []
            for n in high_value_signals:
                hv_meta.append({
                    "id": n.get("id"),
                    "title": n.get("title"),
                    "url": n.get("url"),
                    "source": n.get("source"),
                    "sentiment_score": n.get("sentiment_score"),
                })
            ckpt.save_json("high_value_signals.json", {"count": len(high_value_signals), "items": hv_meta})
        except Exception:
            pass
            
        logger.info(f"--- Step 2: Financial Analysis ({len(high_value_signals)} signals) ---")

        
        analyzed_signals = []

        # Resume from analyzed_signals checkpoint if available
        if resume and ckpt.exists("analyzed_signals.json"):
            logger.info(f"â™»ï¸ Resuming: loading analyzed signals from checkpoint run_id={run_id}")
            analyzed_cached = ckpt.load_json("analyzed_signals.json", default=[])
            if isinstance(analyzed_cached, list) and analyzed_cached:
                analyzed_signals = analyzed_cached
        
        if analyzed_signals:
            logger.info(f"âœ… Using {len(analyzed_signals)} analyzed signals from checkpoint")
        else:

            for signal in high_value_signals:
                logger.info(f"Analyzing: {signal['title']}")

                # 2. æ„é€ ä¸Šä¸‹æ–‡
                content = signal.get("content") or ""
                if len(content) < 50 and signal.get("url"):
                    content = self.trend_agent.news_toolkit.fetch_news_content(signal["url"]) or ""
                input_text = f"ã€{signal['title']}ã€‘\n{content[:3000]}"

                try:
                    # è°ƒç”¨ FinAgent æ‰§è¡Œ ISQ è§£æ
                    sig_obj = self.fin_agent.analyze_signal(input_text, news_id=signal.get("id"))

                    if sig_obj:
                        # è¡¥å……æ¥æºä¿¡æ¯ (å¦‚æœæ¨¡å‹æ²¡å¡«å…¨)
                        if not sig_obj.sources and signal.get("url"):
                            sig_obj.sources = [{"title": signal["title"], "url": signal["url"], "source_name": signal.get("source", "Unknown")}]

                        # ä¿å­˜åˆ°æ·±åº¦ä¿¡å·è¡¨
                        self.db.save_signal(sig_obj.dict())
                        analyzed_signals.append(sig_obj.dict())

                        # åŒæ­¥å› news è¡¨ï¼ˆæ—§é€»è¾‘å…¼å®¹ï¼‰
                        if signal.get("id"):
                            self.db.update_news_content(signal["id"], analysis=sig_obj.summary)

                        # Incremental checkpoint every success to enable resume
                        if len(analyzed_signals) % 3 == 0:
                            ckpt.save_json("analyzed_signals.json", analyzed_signals)
                    else:
                        logger.warning(f"Could not get structured analysis for {signal['title']}, skipping")
                except Exception as e:
                    logger.error(f"Analysis failed for {signal['title']}: {e}")

            ckpt.save_json("analyzed_signals.json", analyzed_signals)

        
        logger.info("--- Step 3: Report Generation ---")

        # Resume from report markdown checkpoint (pre-render)
        if resume and ckpt.exists("report.md"):
            logger.info(f"â™»ï¸ Resuming: using report.md checkpoint for run_id={run_id}")
            md_content = ckpt.load_text("report.md")
        else:
            result = self.report_agent.generate_report(analyzed_signals, user_query=query)
            report = result
            md_content = report.content if hasattr(report, "content") else str(report)
            ckpt.save_text("report.md", md_content)
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = "reports"
        os.makedirs(report_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        md_filename = f"{report_dir}/daily_report_{timestamp}.md"
        
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(md_content)
        
        # è½¬æ¢ä¸º HTML (é»˜è®¤)
        html_filename = save_report_as_html(md_filename)
            
        logger.info(f"âœ… Report generated: {md_filename}")
        if html_filename:
            logger.info(f"ğŸŒ HTML Report available: {html_filename}")
            ckpt.save_json("state.json", {"run_id": run_id, "status": "completed", "finished_at": datetime.now().isoformat(), "output": html_filename})
            return html_filename
        ckpt.save_json("state.json", {"run_id": run_id, "status": "completed", "finished_at": datetime.now().isoformat(), "output": md_filename})
        return md_filename

if __name__ == "__main__":
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description="SignalFlux Workflow - Investment Signal Analysis")
    parser.add_argument("--template", type=str, default="default_isq_v1", 
                        help="ISQ template ID (default: default_isq_v1)")
    parser.add_argument("--sources", type=str, default="all", 
                        help="News sources: 'all', 'financial', 'social', 'tech', or comma-separated list")
    parser.add_argument("--wide", type=int, default=10, 
                        help="Number of news items per source (default: 10)")
    parser.add_argument("--depth", type=str, default="auto", 
                        help="Report depth: 'auto' or integer limit (default: auto)")
    parser.add_argument("--query", type=str, default=None, 
                        help="User query/intent (optional)")
    parser.add_argument("--run-id", type=str, default=None, help="Run id for logs/checkpoints (default: timestamp)")
    parser.add_argument("--resume", action="store_true", help="Resume from latest (or --run-id) checkpoint")
    parser.add_argument("--checkpoint-dir", type=str, default="reports/checkpoints", help="Checkpoint base dir")
    parser.add_argument("--log-dir", type=str, default="logs", help="Log directory")
    parser.add_argument("--log-level", type=str, default="DEBUG", help="Log level (INFO/DEBUG/...) ")
    
    args = parser.parse_args()
    
    # Parse sources
    if args.sources.lower() in ["all", "financial", "social", "tech"]:
        sources = [args.sources.lower()]
    else:
        sources = [s.strip() for s in args.sources.split(",")]
    
    # Parse depth
    try:
        depth = int(args.depth)
    except ValueError:
        depth = args.depth
    
    run_id = args.run_id or make_run_id()
    log_path = setup_file_logging(run_id=run_id, log_dir=args.log_dir, level=args.log_level)
    logger.info(f"ğŸ§¾ Log file: {log_path}")

    workflow = SignalFluxWorkflow(isq_template_id=args.template)
    try:
        workflow.run(
            sources=sources,
            wide=args.wide,
            depth=depth,
            query=args.query,
            run_id=run_id,
            resume=bool(args.resume),
            checkpoint_dir=args.checkpoint_dir,
        )
    except Exception as e:
        # Best-effort crash record
        try:
            ckpt = CheckpointManager(base_dir=args.checkpoint_dir, run_id=run_id)
            ckpt.save_json(
                "state.json",
                {"run_id": run_id, "status": "failed", "error": str(e), "failed_at": datetime.now().isoformat()},
            )
        except Exception:
            pass
        raise
