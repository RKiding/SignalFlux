import os
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Any
from loguru import logger
from dotenv import load_dotenv

from utils.database_manager import DatabaseManager
from utils.llm.factory import get_model
from utils.search_tools import SearchTools
from agents import TrendAgent, FinAgent, ReportAgent, IntentAgent
from schema.models import InvestmentSignal, InvestmentReport
from agno.agent import Agent
from utils.md_to_html import save_report_as_html
from prompts.trend_agent import get_news_filter_instructions

os.environ["NO_PROXY"] = "localhost,127.0.0.1,*.hkust-gz.edu.cn"


class SignalFluxWorkflow:
    """
    SignalFlux ä¸»å·¥ä½œæµ
    
    æµç¨‹:
    1. TrendAgent -> æ‰«æçƒ­ç‚¹
    2. FinAgent -> æ·±åº¦åˆ†æ (å¹¶è¡Œ)
    3. ReportAgent -> ç”Ÿæˆç ”æŠ¥
    """
    
    def __init__(self, db_path: str = "data/signalflux.db"):
        load_dotenv()
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.db = DatabaseManager(db_path)
        
        # åˆå§‹åŒ–æ¨¡å‹
        provider = os.getenv("LLM_PROVIDER", "ust")
        model_id = os.getenv("LLM_MODEL", "Qwen")
        host = os.getenv("OLLAMA_HOST")
        if host:
            self.model = get_model(provider, model_id, host=host)
        else:
            self.model = get_model(provider, model_id)
        
        # åˆå§‹åŒ– Agents
        self.trend_agent = TrendAgent(self.db, self.model, sentiment_mode="bert")
        self.fin_agent = FinAgent(self.db, self.model)
        self.report_agent = ReportAgent(self.db, self.model)
        self.intent_agent = IntentAgent(self.model)
        self.search_tools = SearchTools(self.db)
        
        # ç”¨äºç­›é€‰çš„è½»é‡ Agentï¼ˆä¸å¸¦å·¥å…·ï¼‰
        self.filter_agent = Agent(model=self.model, markdown=True, debug_mode=True)
        
        logger.info("ğŸš€ SignalFlux Workflow initialized")
    
    def _llm_filter_signals(self, news_list: List[Dict], depth: Any, query: str = None) -> List[Dict]:
        """ä½¿ç”¨ LLM æ™ºèƒ½ç­›é€‰é«˜ä»·å€¼ä¿¡å·"""
        if type(depth) == int and len(news_list) <= depth and not query:
            return news_list
        
        # æ„å»ºæ–°é—»åˆ—è¡¨æ–‡æœ¬
        news_text = "\n".join([
            f"[ID: {n.get('id', i)}] {n['title']} (æƒ…ç»ª: {n.get('sentiment_score', 'N/A')})"
            for i, n in enumerate(news_list)
        ])
        
        # ç”Ÿæˆç­›é€‰ prompt (å¸¦ query)
        filter_instruction = get_news_filter_instructions(len(news_list), depth, query)
        self.filter_agent.instructions = [filter_instruction]
        
        try:
            response = self.filter_agent.run(f"è¯·ç­›é€‰ä»¥ä¸‹æ–°é—»:\n{news_text}")
            content = response.content
            
            # æå– JSON
            from utils.json_utils import extract_json
            result = extract_json(content)
            if not result:
                logger.warning(f"Failed to parse LLM filter response: {content}")
                return news_list
            
            selected_ids = result.get("selected_ids", [])
            themes = result.get("themes", [])
            
            logger.info(f"ğŸ¯ LLM ç­›é€‰ç»“æœ: {len(selected_ids)} æ¡, {len(themes)} ä¸ªä¸»é¢˜")
            
            # æ ¹æ® ID ç­›é€‰æ–°é—»
            id_set = set(str(sid) for sid in selected_ids)
            filtered = [n for n in news_list if str(n.get('id', '')) in id_set]
            
            # åŠ¨æ€é€»è¾‘ï¼š
            # 1. åªæœ‰åœ¨ LLM æœªé€‰å‡ºä»»ä½•å†…å®¹ä¸”éç‰¹å®šæŸ¥è¯¢æ—¶ï¼Œæ‰å›é€€åˆ°é»˜è®¤å‰å‡ æ¡
            if not filtered and not query:
                 logger.warning("âš ï¸ LLM selected 0 items, falling back to top items")
                 return news_list
            
            # 2. å¦‚æœæœ‰ queryï¼Œå®Œå…¨ä¿¡ä»» LLM çš„é€‰æ‹©ï¼ˆæ•°é‡å¯èƒ½å°‘äºæˆ–å¤šäº depthï¼‰
            if query:
                return filtered
            
            # 3. å¦‚æœæ˜¯é€šç”¨æ‰«æï¼Œé™åˆ¶æœ€å¤§è¿”å›æ•°é‡
            return filtered
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLM ç­›é€‰å¤±è´¥: {e}, å›é€€åˆ°å…¨éƒ¨è¿”å›")
            return news_list

    # å¯ç”¨çš„æ–°é—»æºï¼ˆæŒ‰ç±»åˆ«ï¼‰
    FINANCIAL_SOURCES = ["cls", "wallstreetcn", "xueqiu", "eastmoney", "yicai"]
    SOCIAL_SOURCES = ["weibo", "zhihu", "baidu", "toutiao", "douyin"]
    TECH_SOURCES = ["36kr", "ithome", "v2ex", "juejin", "hackernews"]
    ALL_SOURCES = FINANCIAL_SOURCES + SOCIAL_SOURCES + TECH_SOURCES
    
    def run(self, sources: List[str] = ["all"], wide: int = 10, depth: Any = 'auto', query: Optional[str] = None):
        """æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            sources: æ–°é—»æ¥æºåˆ—è¡¨
            wide:  æ–°é—»æŠ“å–å¹¿åº¦ï¼ˆæ¯ä¸ªæºæŠ“å–çš„æ•°é‡ï¼‰
            depth: ç”ŸæˆæŠ¥å‘Šçš„æ·±åº¦ï¼Œè‹¥ä¸ºautoï¼Œåˆ™ç”±LLMæ€»ç»“åˆ¤æ–­ï¼Œè‹¥ä¸ºæ•´æ•°åˆ™é™åˆ¶æœ€åç”Ÿæˆçš„ä¿¡å·æ•°é‡
            query:  ç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚ "é¦™æ¸¯ç«ç¾"ã€"Aè‚¡ç§‘æŠ€æ¿å—"
        """
        logger.info("--- Step 1: Trend Discovery ---")
        
        # 0. æ„å›¾åˆ†æ (å¦‚æœå­˜åœ¨ query)
        intent_info = ""
        if query:
            logger.info(f"ğŸ§  Analyzing intent for: {query}")
            intent_info = self.intent_agent.run(query)
        
        # 1. è§£æ sources å‚æ•°
        if "all" in sources:
            actual_sources = self.ALL_SOURCES.copy()
        elif "financial" in sources:
            actual_sources = self.FINANCIAL_SOURCES.copy()
        elif "social" in sources:
            actual_sources = self.SOCIAL_SOURCES.copy()
        elif "tech" in sources:
            actual_sources = self.TECH_SOURCES.copy()
        else:
            actual_sources = sources
        
        logger.info(f"ğŸ“¡ Attempting to fetch from {len(actual_sources)} sources...")
        
        # 2. è·å–çƒ­ç‚¹
        successful_sources = []
        for source in actual_sources:
            try:
                # ä½¿ç”¨ wide æ§åˆ¶æŠ“å–æ•°é‡
                result = self.trend_agent.news_toolkit.fetch_hot_news(source, count=wide)
                if result and len(result) > 0:
                    successful_sources.append(source)
                else:
                    logger.warning(f"âš ï¸ Source '{source}' returned no data, skipping")
            except Exception as e:
                logger.warning(f"âš ï¸ Source '{source}' failed: {e}, skipping")
        
        logger.info(f"âœ… Successfully fetched from {len(successful_sources)}/{len(actual_sources)} sources")
            
        # --- NEW: Active Search based on Intent ---
        search_signals = []
        if query and isinstance(intent_info, dict):
            search_queries = intent_info.get("search_queries", [query])
            is_specific = intent_info.get("is_specific_event", False)
            
            # å¦‚æœæ˜¯ç‰¹å®šäº‹ä»¶ï¼Œæˆ–è€…ç”¨æˆ·æ˜ç¡®æé—®ï¼Œæˆ‘ä»¬åº”è¯¥ä¸»åŠ¨æœç´¢
            if is_specific or len(search_queries) > 0:
                logger.info(f"ğŸ” Executing active search for queries: {search_queries}")
                for q in search_queries[:2]: # é™åˆ¶æŸ¥è¯¢æ•°ï¼Œé¿å…å¤ªæ…¢
                    # Consider using 'baidu' for Chinese queries if 'ddg' is unstable
                    # enrich=True is default, so we get full content
                    results = self.search_tools.search_list(q, engine="baidu", max_results=5, enrich=True)
                    for r in results:
                        # è½¬æ¢ä¸ºæ ‡å‡†ä¿¡å·æ ¼å¼ (search_tools now returns standard keys including id, rank, etc)
                        search_signals.append({
                            "title": r.get('title'),
                            "url": r.get('url'),
                            "source": r.get('source', 'Search'), # keeping original source name
                            "content": r.get('content'),
                            "publish_time": r.get('publish_time') or datetime.now(), 
                            "sentiment_score": r.get('sentiment_score', 0), 
                            "id": r.get('id') or f"search_{hash(r.get('url'))}"
                        })
                logger.info(f"ğŸ” Found {len(search_signals)} signals via search")

        # 2. æ‰¹é‡æ›´æ–°æƒ…ç»ªåˆ†æ•° (ä¿ç•™ï¼Œç”¨äºå¯è§†åŒ–)
        logger.info("Calculating sentiment scores...")
        self.trend_agent.sentiment_toolkit.batch_update_sentiment(limit=50)
        
        # 3. ä»æ•°æ®åº“è¯»å–æ–°é—» + åˆå¹¶æœç´¢ç»“æœ
        db_news = self.db.get_daily_news(limit=50)
        
        # åˆå¹¶åˆ—è¡¨ (Search signals preferred if query exists)
        raw_news = search_signals + db_news if search_signals else db_news
        
        if not raw_news:
            logger.warning("No news found in database.")
            return
        
        # 4. æ™ºèƒ½ç­›é€‰ï¼ˆLLM æˆ–ä¼ ç»Ÿæ–¹å¼ï¼‰
        # å¦‚æœæœ‰ queryï¼Œå³ä½¿æ•°é‡å°‘ä¹Ÿå»ºè®®èµ° LLM ç­›é€‰ä»¥åŒ¹é…ç›¸å…³æ€§
        if depth == 'auto' or query:
            logger.info(f"ğŸ¤– Using LLM to filter signals (Query: {query if query else 'Default'})...")
            high_value_signals = self._llm_filter_signals(raw_news, depth, query)
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šæŒ‰æƒ…ç»ªåˆ†æ•°æ’åº
            if type(depth) == int and depth>0:
                high_value_signals = sorted(
                    raw_news, 
                    key=lambda x: abs(x.get("sentiment_score") or 0), 
                    reverse=True
                )[:depth]
            else:
                high_value_signals = raw_news
            
        logger.info(f"--- Step 2: Financial Analysis ({len(high_value_signals)} signals) ---")

        
        analyzed_signals = []
        
        for signal in high_value_signals:
            logger.info(f"Analyzing: {signal['title']}")
            
            # 1. ä¼˜å…ˆä»æ•°æ®åº“ä¸­å¯»æ‰¾åŒä¸€ signal_id çš„æ·±åº¦è§£æ (ISQ)
            # è¿™é‡Œå¯ä»¥ç”¨ä¸€ä¸ªä¸“é—¨çš„ get_signal æ–¹æ³•ï¼Œç›®å‰æˆ‘ä»¬å…ˆçœ‹ news è¡¨æ˜¯å¦æœ‰ analysis å­—æ®µç¼“å­˜ï¼ˆæ—§é€»è¾‘é©±åŠ¨ï¼‰
            # æˆ–è€…ç›´æ¥çœ‹ signals è¡¨
            
            # 2. æ„é€ ä¸Šä¸‹æ–‡
            content = signal.get("content") or ""
            if len(content) < 50 and signal.get("url"):
                content = self.trend_agent.news_toolkit.fetch_news_content(signal["url"]) or ""
            input_text = f"ã€{signal['title']}ã€‘\n{content[:3000]}"
            
            try:
                # è°ƒç”¨ FinAgent æ‰§è¡Œ ISQ è§£æ
                sig_obj = self.fin_agent.analyze_signal(input_text, news_id=signal.get("id"))
                
                if sig_obj:
                    # è¡¥å……æ¥æºä¿¡æ¯ (å¦‚æœæ¨¡å‹æ²¡å¡«å…¨)
                    if not sig_obj.sources and signal.get("url"):
                        sig_obj.sources = [{"title": signal["title"], "url": signal["url"], "source_name": signal.get("source", "Unknown")}]
                    
                    # ä¿å­˜åˆ°æ·±åº¦ä¿¡å·è¡¨
                    self.db.save_signal(sig_obj.dict())
                    analyzed_signals.append(sig_obj)
                    
                    # åŒæ­¥å› news è¡¨ï¼ˆæ—§é€»è¾‘å…¼å®¹ï¼‰
                    if signal.get("id"):
                        self.db.update_news_content(signal["id"], analysis=sig_obj.summary)
                else:
                    logger.warning(f"Could not get structured analysis for {signal['title']}, skipping")
            except Exception as e:
                logger.error(f"Analysis failed for {signal['title']}: {e}")

        
        logger.info("--- Step 3: Report Generation ---")
        
        result = self.report_agent.generate_report(analyzed_signals, user_query=query)
        # Use a generic name 'report' but note it might be a string now (generate_report returns str)
        report = result
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = "reports"
        os.makedirs(report_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        md_filename = f"{report_dir}/daily_report_{timestamp}.md"
        
        with open(md_filename, "w", encoding="utf-8") as f:
            # Handle both RunResponse object and raw string
            md_content = report.content if hasattr(report, "content") else str(report)
            f.write(md_content)
        
        # è½¬æ¢ä¸º HTML (é»˜è®¤)
        html_filename = save_report_as_html(md_filename)
            
        logger.info(f"âœ… Report generated: {md_filename}")
        if html_filename:
            logger.info(f"ğŸŒ HTML Report available: {html_filename}")
            return html_filename
        return md_filename

if __name__ == "__main__":
    workflow = SignalFluxWorkflow()
    workflow.run(query='å¸®æˆ‘åˆ†æä¸€ä¸‹è¿‘æœŸçƒ­ç‚¹')
    # workflow.run(sources=['social'], wide=5, depth='auto')
