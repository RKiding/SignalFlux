import requests
from requests.exceptions import RequestException, Timeout
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
from loguru import logger
from utils.database_manager import DatabaseManager
from utils.content_extractor import ContentExtractor

class NewsNowTools:
    """çƒ­ç‚¹æ–°é—»è·å–å·¥å…· - æ¥å…¥ NewsNow API ä¸ Jina å†…å®¹æå–"""
    
    BASE_URL = "https://newsnow.busiyi.world"
    SOURCES = {
        # é‡‘èç±»
        "cls": "è´¢è”ç¤¾",
        "wallstreetcn": "åå°”è¡—è§é—»",
        "xueqiu": "é›ªçƒçƒ­æ¦œ",
        # ç»¼åˆ/ç¤¾äº¤
        "weibo": "å¾®åšçƒ­æœ",
        "zhihu": "çŸ¥ä¹çƒ­æ¦œ",
        "baidu": "ç™¾åº¦çƒ­æœ",
        "toutiao": "ä»Šæ—¥å¤´æ¡",
        "douyin": "æŠ–éŸ³çƒ­æ¦œ",
        "thepaper": "æ¾æ¹ƒæ–°é—»",
        # ç§‘æŠ€ç±»
        "36kr": "36æ°ª",
        "ithome": "ITä¹‹å®¶",
        "v2ex": "V2EX",
        "juejin": "æ˜é‡‘",
        "hackernews": "Hacker News",
    }


    def __init__(self, db: DatabaseManager):
        self.db = db
        self.user_agent = (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        )
        self.extractor = ContentExtractor()
        # Simple in-memory cache: source_id -> {"time": timestamp, "data": []}
        self._cache = {}

    def fetch_hot_news(self, source_id: str, count: int = 15, fetch_content: bool = False) -> List[Dict]:
        """
        ä»æŒ‡å®šæ–°é—»æºè·å–çƒ­ç‚¹æ–°é—»åˆ—è¡¨ï¼ˆæ”¯æŒ5åˆ†é’Ÿç¼“å­˜ï¼‰ã€‚
        """
        # 1. Check cache validity (5 minutes)
        cache_key = f"{source_id}_{count}"
        cached = self._cache.get(cache_key)
        now = time.time()
        
        if cached and (now - cached["time"] < 300):
            logger.info(f"âš¡ Using cached news for {source_id} (Age: {int(now - cached['time'])}s)")
            return cached["data"]

        try:
            url = f"{self.BASE_URL}/api/s?id={source_id}"
            response = requests.get(url, headers={"User-Agent": self.user_agent}, timeout=30)
            if response.status_code == 200:
                data = response.json()
                items = data.get("items", [])[:count]
                processed_items = []
                for i, item in enumerate(items, 1):
                    item_url = item.get("url", "")
                    content = ""
                    if fetch_content and item_url:
                        content = self.extractor.extract_with_jina(item_url) or ""
                    
                    processed_items.append({
                        "id": item.get("id") or f"{source_id}_{int(time.time())}_{i}",
                        "source": source_id,
                        "rank": i,
                        "title": item.get("title", ""),
                        "url": item_url,
                        "content": content,
                        "publish_time": item.get("publish_time"),
                        "meta_data": item.get("extra", {})
                    })
                
                # Update Cache
                self._cache[cache_key] = {"time": now, "data": processed_items}
                logger.info(f"âœ… Fetched and cached news for {source_id}")
                
                self.db.save_daily_news(processed_items)
                return processed_items
            else:
                logger.error(f"NewsNow API Error: {response.status_code}")
                # Fallback to stale cache if available
                if cached:
                    logger.warning(f"âš ï¸ API failed, using stale cache for {source_id}")
                    return cached["data"]
                return []
        except Timeout:
            logger.error(f"Timeout fetching hot news from {source_id}")
            if cached:
                logger.warning(f"âš ï¸ Timeout, using stale cache for {source_id}")
                return cached["data"]
            return []
        except RequestException as e:
            logger.error(f"Network error fetching hot news from {source_id}: {e}")
            if cached:
                 logger.warning(f"âš ï¸ Network check failed, using stale cache for {source_id}")
                 return cached["data"]
            return []
        except json.JSONDecodeError:
            logger.error(f"Failed to parse JSON response from NewsNow for {source_id}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error fetching hot news from {source_id}: {e}")
            return []

    def fetch_news_content(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨ Jina Reader æŠ“å–æŒ‡å®š URL çš„ç½‘é¡µæ­£æ–‡å†…å®¹ã€‚
        
        Args:
            url: éœ€è¦æŠ“å–å†…å®¹çš„å®Œæ•´ç½‘é¡µ URLï¼Œå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´ã€‚
        
        Returns:
            æå–çš„ç½‘é¡µæ­£æ–‡å†…å®¹ (Markdown æ ¼å¼)ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› Noneã€‚
        """
        return self.extractor.extract_with_jina(url)

    def get_unified_trends(self, sources: Optional[List[str]] = None) -> str:
        """
        è·å–å¤šå¹³å°ç»¼åˆçƒ­ç‚¹æŠ¥å‘Šï¼Œè‡ªåŠ¨èšåˆå¤šä¸ªæ–°é—»æºçš„çƒ­é—¨å†…å®¹ã€‚
        
        Args:
            sources: è¦æ‰«æçš„æ–°é—»æºåˆ—è¡¨ã€‚å¯é€‰å€¼æŒ‰ç±»åˆ«:
                **é‡‘èç±»**: "cls", "wallstreetcn", "xueqiu"
                **ç»¼åˆç±»**: "weibo", "zhihu", "baidu", "toutiao", "douyin", "thepaper"
                **ç§‘æŠ€ç±»**: "36kr", "ithome", "v2ex", "juejin", "hackernews"
        
        Returns:
            æ ¼å¼åŒ–çš„ Markdown çƒ­ç‚¹æ±‡æ€»æŠ¥å‘Šï¼ŒåŒ…å«å„å¹³å° Top 10 çƒ­ç‚¹æ ‡é¢˜å’Œé“¾æ¥ã€‚
        """
        sources = sources or ["weibo", "zhihu", "wallstreetcn"]
        all_news = []
        for src in sources:
            all_news.extend(self.fetch_hot_news(src))
            time.sleep(0.2)
        
        if not all_news:
            return "âŒ æœªèƒ½è·å–åˆ°çƒ­ç‚¹æ•°æ®"
            
        report = f"# å®æ—¶å…¨ç½‘çƒ­ç‚¹æ±‡æ€» ({datetime.now().strftime('%Y-%m-%d %H:%M')})\n\n"
        for src in sources:

            src_name = self.SOURCES.get(src, src)
            report += f"### ğŸ”¥ {src_name}\n"
            src_news = [n for n in all_news if n['source'] == src]
            for n in src_news[:10]:
                report += f"- {n['title']} ([é“¾æ¥]({n['url']}))\n"
            report += "\n"
            
        return report


class PolymarketTools:
    """Polymarket é¢„æµ‹å¸‚åœºæ•°æ®å·¥å…· - è·å–çƒ­é—¨é¢„æµ‹å¸‚åœºåæ˜ å…¬ä¼—æƒ…ç»ªå’Œé¢„æœŸ"""
    
    BASE_URL = "https://gamma-api.polymarket.com"
    
    def __init__(self, db: DatabaseManager):
        self.db = db
        self.user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    
    def get_active_markets(self, limit: int = 20) -> List[Dict]:
        """
        è·å–æ´»è·ƒçš„é¢„æµ‹å¸‚åœºï¼Œç”¨äºåˆ†æå…¬ä¼—æƒ…ç»ªå’Œé¢„æœŸã€‚
        
        é¢„æµ‹å¸‚åœºæ•°æ®å¯ä»¥åæ˜ :
        - å…¬ä¼—å¯¹é‡å¤§äº‹ä»¶çš„é¢„æœŸæ¦‚ç‡
        - å¸‚åœºæƒ…ç»ªå’Œé£é™©åå¥½
        - çƒ­é—¨è¯é¢˜çš„å…³æ³¨åº¦
        
        Args:
            limit: è·å–çš„å¸‚åœºæ•°é‡ï¼Œé»˜è®¤ 20 ä¸ªã€‚
        
        Returns:
            åŒ…å«é¢„æµ‹å¸‚åœºä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå¸‚åœºåŒ…å«:
            - question: é¢„æµ‹é—®é¢˜
            - outcomes: å¯èƒ½çš„ç»“æœ
            - outcomePrices: å„ç»“æœçš„æ¦‚ç‡ä»·æ ¼
            - volume: äº¤æ˜“é‡
        """
        try:
            response = requests.get(
                f"{self.BASE_URL}/markets",
                params={"active": "true", "closed": "false", "limit": limit},
                headers={"User-Agent": self.user_agent, "Accept": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                markets = response.json()
                result = []
                for m in markets:
                    result.append({
                        "id": m.get("id"),
                        "question": m.get("question"),
                        "slug": m.get("slug"),
                        "outcomes": m.get("outcomes"),
                        "outcomePrices": m.get("outcomePrices"),
                        "volume": m.get("volume"),
                        "liquidity": m.get("liquidity"),
                    })
                logger.info(f"âœ… è·å– {len(result)} ä¸ªé¢„æµ‹å¸‚åœº")
                return result
            else:
                logger.warning(f"âš ï¸ Polymarket API è¿”å› {response.status_code}")
                return []
        except Timeout:
            logger.error("Timeout fetching Polymarket markets")
            return []
        except RequestException as e:
            logger.error(f"Network error fetching Polymarket markets: {e}")
            return []
        except json.JSONDecodeError:
            logger.error("Failed to parse JSON response from Polymarket")
            return []
        except Exception as e:
            logger.error(f"Unexpected error fetching Polymarket markets: {e}")
            return []
    
    def get_market_summary(self, limit: int = 10) -> str:
        """
        è·å–é¢„æµ‹å¸‚åœºæ‘˜è¦æŠ¥å‘Šï¼Œç”¨äºäº†è§£å½“å‰çƒ­é—¨è¯é¢˜å’Œå…¬ä¼—é¢„æœŸã€‚
        
        Args:
            limit: è·å–çš„å¸‚åœºæ•°é‡
            
        Returns:
            æ ¼å¼åŒ–çš„é¢„æµ‹å¸‚åœºæŠ¥å‘Š
        """
        markets = self.get_active_markets(limit)
        if not markets:
            return "âŒ æ— æ³•è·å– Polymarket æ•°æ®"
        
        report = f"# ğŸ”® Polymarket çƒ­é—¨é¢„æµ‹ ({datetime.now().strftime('%Y-%m-%d %H:%M')})\n\n"
        for i, m in enumerate(markets, 1):
            question = m.get("question", "Unknown")
            prices = m.get("outcomePrices", [])
            volume = m.get("volume", 0)
            
            report += f"**{i}. {question}**\n"
            if prices:
                report += f"   æ¦‚ç‡: {prices}\n"
            if volume:
                report += f"   äº¤æ˜“é‡: ${float(volume):,.0f}\n"
            report += "\n"
        
        return report
