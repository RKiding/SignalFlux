import os
import hashlib
import json
import re
from typing import List, Dict, Optional, Any
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.baidusearch import BaiduSearchTools
from agno.agent import Agent
from loguru import logger
from datetime import datetime
from utils.database_manager import DatabaseManager
from utils.content_extractor import ContentExtractor
from utils.llm.factory import get_model
from utils.hybrid_search import LocalNewsSearch

# é»˜è®¤æœç´¢ç¼“å­˜ TTLï¼ˆç§’ï¼‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
DEFAULT_SEARCH_TTL = int(os.getenv("SEARCH_CACHE_TTL", "3600"))  # é»˜è®¤ 1 å°æ—¶

class SearchTools:
    """æ‰©å±•æ€§æœç´¢å·¥å…·åº“ - æ”¯æŒå¤šå¼•æ“èšåˆä¸å†…å®¹ç¼“å­˜"""
    
    def __init__(self, db: DatabaseManager):
        self.db = db
        self._engines = {
            "ddg": DuckDuckGoTools(),
            "baidu": BaiduSearchTools(),
            "local": LocalNewsSearch(db)
        }

    def _generate_hash(self, query: str, engine: str, max_results: int) -> str:
        return hashlib.md5(f"{engine}:{query}:{max_results}".encode()).hexdigest()

    def search(self, query: str, engine: str = "ddg", max_results: int = 5, ttl: Optional[int] = None) -> str:
        """
        ä½¿ç”¨æŒ‡å®šæœç´¢å¼•æ“æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œç»“æœä¼šè¢«ç¼“å­˜ä»¥æé«˜æ•ˆç‡ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼Œå¦‚ "è‹±ä¼Ÿè¾¾è´¢æŠ¥" æˆ– "å…‰ä¼è¡Œä¸šæ”¿ç­–"ã€‚
            engine: æœç´¢å¼•æ“é€‰æ‹©ã€‚å¯é€‰å€¼: "ddg" (DuckDuckGoï¼Œæ¨èè‹±æ–‡/å›½é™…æœç´¢), 
                    "baidu" (ç™¾åº¦ï¼Œæ¨èä¸­æ–‡/å›½å†…æœç´¢),
                    "local" (æœ¬åœ°å†å²æ–°é—»æœç´¢ï¼ŒåŸºäºå‘é‡+BM25)ã€‚é»˜è®¤ "ddg"ã€‚
            max_results: æœŸæœ›è¿”å›çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ 5 æ¡ã€‚
            ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰ã€‚å¦‚æœç¼“å­˜è¶…è¿‡æ­¤æ—¶é—´ä¼šé‡æ–°æœç´¢ã€‚
                 é»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡ SEARCH_CACHE_TTL æˆ– 3600 ç§’ã€‚
                 è®¾ä¸º 0 å¯å¼ºåˆ¶åˆ·æ–°ã€‚
        
        Returns:
            æœç´¢ç»“æœçš„æ–‡æœ¬æè¿°ï¼ŒåŒ…å«æ ‡é¢˜ã€æ‘˜è¦å’Œé“¾æ¥ã€‚
        """
        if engine not in self._engines:
            return f"Error: Unsupported engine '{engine}'. Available: {list(self._engines.keys())}"

        query_hash = self._generate_hash(query, engine, max_results)
        effective_ttl = ttl if ttl is not None else DEFAULT_SEARCH_TTL
        
        # 1. å°è¯•ä»ç¼“å­˜è¯»å– (local å¼•æ“ä¸ç¼“å­˜ï¼Œå› ä¸ºå®ƒæœ¬èº«å°±æ˜¯æŸ¥åº“)
        if engine != "local":
            cache = self.db.get_search_cache(query_hash, ttl_seconds=effective_ttl if effective_ttl > 0 else None)
            if cache and effective_ttl != 0:
                logger.info(f"â„¹ï¸ Found search results in cache for: {query} ({engine})")
                return cache['results']

        # 2. æ‰§è¡ŒçœŸå®æœç´¢
        logger.info(f"ğŸ“¡ Searching {engine} for: {query}")
        try:
            tool = self._engines[engine]
            if engine == "ddg":
                results = tool.duckduckgo_search(query, max_results=max_results)
            elif engine == "baidu":
                results = tool.baidu_search(query, max_results=max_results)
            elif engine == "local":
                # LocalNewsSearch è¿”å›çš„æ˜¯ List[Dict]
                local_results = tool.search(query, top_n=max_results)
                results = []
                for r in local_results:
                    results.append({
                        "title": r.get("title"),
                        "href": r.get("url", "local"),
                        "body": r.get("content", "")[:300]
                    })
            else:
                results = "Search not implemented for this engine."
            
            results_str = str(results)
            if engine != "local":
                self.db.save_search_cache(query_hash, query, engine, results_str)
            return results_str
            
        except Exception as e:
            logger.error(f"âŒ Search failed for {query}: {e}")
            return f"Error occurred during search: {str(e)}"

    def search_list(self, query: str, engine: str = "ddg", max_results: int = 5, ttl: Optional[int] = None, enrich: bool = True) -> List[Dict]:
        """
        æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æ„åŒ–åˆ—è¡¨ (List[Dict])ã€‚
        Dict åŒ…å«: title, href (or url), body (or snippet)
        
        Args:
            enrich: æ˜¯å¦æŠ“å–æ­£æ–‡å†…å®¹ (é»˜è®¤ True)
        """
        if engine not in self._engines:
            logger.error(f"Unsupported engine {engine}")
            return []
            
        # ä¸åŒçš„ hash ä»¥åŒºåˆ†æ˜¯å¦ enrichment
        enrich_suffix = ":enriched" if enrich else ""
        query_hash = self._generate_hash(query, engine + enrich_suffix, max_results)
        effective_ttl = ttl if ttl is not None else DEFAULT_SEARCH_TTL
        
        # 1. å°è¯•ä»ç¼“å­˜è¯»å–
        cache = self.db.get_search_cache(query_hash, ttl_seconds=effective_ttl if effective_ttl > 0 else None)
        if cache and effective_ttl != 0:
            try:
                cached_data = json.loads(cache['results'])
                if isinstance(cached_data, list):
                    logger.info(f"â„¹ï¸ Found structured search cache for: {query}")
                    return cached_data
            except:
                pass
        
        # 1.5 Smart Cache (Fuzzy + LLM)
        if effective_ttl != 0:
            try:
                # 1. Similar cached queries
                similar_queries = self.db.find_similar_queries(query, limit=3)
                # Filter by TTL
                valid_candidates = []
                for q in similar_queries:
                    if q['query'] == query: continue 
                    q_time = datetime.fromisoformat(q['timestamp'])
                    if effective_ttl and (datetime.now() - q_time).total_seconds() > effective_ttl:
                        continue
                    q['type'] = 'cached_search'
                    valid_candidates.append(q)

                # 2. Relevant local news (as search results)
                local_news = self.db.search_local_news(query, limit=3)
                if local_news:
                    # Group local news as a single "candidate" source? Or individual?
                    # Better to treat "Local News Database" as one candidate source that contains X items.
                    # Or just add them to candidates list?
                    # Let's package strictly relevant news as a "local_news_bundle"
                    valid_candidates.append({
                        'type': 'local_news',
                        'query': 'Local Database News',
                        'items': local_news,
                        'timestamp': datetime.now().isoformat()
                    })
                
                if valid_candidates:
                    logger.info(f"ğŸ¤” Found {len(valid_candidates)} smart cache candidates (Queries/News). Asking LLM...")
                    evaluation = self._evaluate_cache_relevance(query, valid_candidates)
                    
                    if evaluation and evaluation.get('reuse', False):
                        idx = evaluation.get('index', -1)
                        if 0 <= idx < len(valid_candidates):
                            chosen = valid_candidates[idx]
                            logger.info(f"ğŸ¤– LLM suggested reusing: '{chosen.get('query')}' ({chosen['type']})")
                            
                            if chosen['type'] == 'cached_search':
                                # Load the chosen cache
                                cache = self.db.get_search_cache(chosen['query_hash']) 
                                if cache:
                                    try:
                                        cached_data = json.loads(cache['results'])
                                        if isinstance(cached_data, list):
                                            return cached_data
                                    except:
                                        pass
                            elif chosen['type'] == 'local_news':
                                # Convert local news items to search result format
                                news_results = []
                                for i, news in enumerate(chosen['items'], 1):
                                    news_results.append({
                                        "id": news.get('id'),
                                        "rank": i,
                                        "title": news.get('title'),
                                        "url": news.get('url'),
                                        "content": news.get('content'),
                                        "original_snippet": news.get('content')[:200] if news.get('content') else '',
                                        "source": f"Local News ({news.get('source')})",
                                        "publish_time": news.get('publish_time'),
                                        "crawl_time": news.get('crawl_time'),
                                        "sentiment_score": news.get('sentiment_score', 0),
                                        "meta_data": {"origin": "local_db"}
                                    })
                                return news_results

            except Exception as e:
                logger.warning(f"Smart cache check failed: {e}")
        
        # 2. æ‰§è¡Œæœç´¢
        logger.info(f"ğŸ“¡ Searching {engine} (structured) for: {query}")
        try:
            tool = self._engines[engine]
            results = []
            if engine == "ddg":
                results = tool.duckduckgo_search(query, max_results=max_results)
            elif engine == "baidu":
                results = tool.baidu_search(query, max_results=max_results)
            elif engine == "local":
                # LocalNewsSearch è¿”å›çš„æ˜¯ List[Dict]
                local_results = tool.search(query, top_n=max_results)
                results = []
                for r in local_results:
                    results.append({
                        "title": r.get("title"),
                        "url": r.get("url", "local"),
                        "body": r.get("content", "")[:500],
                        "source": f"Local ({r.get('source', 'db')})",
                        "publish_time": r.get("publish_time")
                    })
            
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ JSON è¿”å› (Baidu å¸¸è¿” JSON å­—ç¬¦ä¸²)
            if isinstance(results, str) and engine != "local":
                try:
                    results = json.loads(results)
                except:
                    pass
            
            # è½¬ä¸ºç»Ÿä¸€æ ¼å¼
            normalized_results = []
            if isinstance(results, list):
                
                for i, r in enumerate(results, 1):
                    title = r.get('title', '')
                    url = r.get('href') or r.get('url') or r.get('link', '')
                    content = r.get('body') or r.get('snippet') or r.get('abstract', '')
                    
                    if title and url:
                        normalized_results.append({
                            "id": self._generate_hash(url + query, "search_item", i),
                            "rank": i,
                            "title": title,
                            "url": url,
                            "content": content,
                            "original_snippet": content, # ä¿ç•™æ‘˜è¦
                            "source": f"Search ({engine})",
                            "publish_time": datetime.now().isoformat(), # æš‚ç”¨å½“å‰æ—¶é—´
                            "crawl_time": datetime.now().isoformat(),
                            "meta_data": {"query": query, "engine": engine}
                        })
            
            # Fallback if still string and failed to parse
            elif isinstance(results, str) and results:
                 normalized_results.append({"title": query, "url": "", "content": results, "source": engine})

            # 3. æŠ“å–æ­£æ–‡ & è®¡ç®—æƒ…ç»ª (Enrichment)
            if enrich and normalized_results:
                logger.info(f"ğŸ•¸ï¸ Enriching {len(normalized_results)} search results with Jina & Sentiment...")
                extractor = ContentExtractor()
                
                # Lazy load sentiment tool
                if not hasattr(self, 'sentiment_tool') or self.sentiment_tool is None:
                    from utils.sentiment_tools import SentimentTools
                    self.sentiment_tool = SentimentTools(self.db)
                
                for item in normalized_results:
                    if item.get("url"):
                        try:
                            # Use Jina to get full content
                            full_content = extractor.extract_with_jina(item["url"], timeout=60)
                            if full_content and len(full_content) > 100:
                                item["content"] = full_content
                                
                                # Calculate sentiment
                                # Use title + snippet of content for efficiency
                                text_to_analyze = f"{item['title']} {full_content[:500]}"
                                sent_result = self.sentiment_tool.analyze_sentiment(text_to_analyze)  # Using self.sentiment_tool
                                score = sent_result.get('score', 0.0)
                                item["sentiment_score"] = float(score)
                                
                                logger.info(f"  âœ… Enriched: {item['title'][:20]}... (Sentiment: {score:.2f})")
                            else:
                                # Fallback: Use snippet for sentiment
                                logger.info(f"  âš ï¸ Content short/failed for {item['url']}, using snippet for sentiment.")
                                text_to_analyze = f"{item['title']} {item['content']}" # content is snippet here
                                sent_result = self.sentiment_tool.analyze_sentiment(text_to_analyze)
                                score = sent_result.get('score', 0.0)
                                item["sentiment_score"] = float(score)

                        except Exception as e:
                             # Fallback: Use snippet for sentiment on error
                            logger.warning(f"Failed to enrich {item['url']}: {e}. Using snippet.")
                            text_to_analyze = f"{item['title']} {item['content']}"
                            sent_result = self.sentiment_tool.analyze_sentiment(text_to_analyze)
                            score = sent_result.get('score', 0.0)
                            item["sentiment_score"] = float(score)
            
            # ç¼“å­˜ç»“æœ list
            if normalized_results:
                # Pass list directly, DB manager will handle JSON dump for main cache and populate search_details
                # Only cache if NOT from local news reuse (though this logic path is for fresh search)
                self.db.save_search_cache(query_hash, query, engine, normalized_results)
            
            return normalized_results
            
        except Exception as e:
            logger.error(f"âŒ Structured search failed for {query}: {e}")
            return []

    def _evaluate_cache_relevance(self, current_query: str, candidates: List[Dict]) -> Dict:
        """
        ä½¿ç”¨ LLM è¯„ä¼°ç¼“å­˜å€™é€‰æ˜¯å¦è¶³ä»¥å›ç­”å½“å‰é—®é¢˜ã€‚
        """
        try:
            # Prepare candidates text
            candidates_desc = []
            for i, c in enumerate(candidates):
                if c['type'] == 'cached_search':
                    # Preview cached results if available? 
                    # Maybe just use the query string as a proxy for what's in there.
                    # Or peek at 'results' snippet.
                    preview = ""
                    try:
                         # Attempt to peek first result title from JSON string
                         # Note: c.get('results') might be a stringified JSON list
                         res_list = json.loads(c.get('results', '[]'))
                         if res_list and isinstance(res_list, list) and len(res_list) > 0:
                             first_item = res_list[0]
                             if isinstance(first_item, dict) and 'title' in first_item:
                                 preview = f" (Contains: {first_item.get('title', '')[:50]}...)"
                    except:
                        pass
                    candidates_desc.append(f"[{i}] Old Search Query: '{c['query']}' {preview} (Time: {c['timestamp']})")
                elif c['type'] == 'local_news':
                     # List titles of local news
                     titles = [item['title'] for item in c['items'][:3]]
                     candidates_desc.append(f"[{i}] Local Database News: {', '.join(titles)}... (Time: {c['timestamp']})")

            prompt = f"""
            Task: Decide if existing information is sufficient for the new search query.
            
            New Query: "{current_query}"
            
            Available Information Candidates:
            {chr(10).join(candidates_desc)}
            
            Instructions:
            1. Analyze if any candidate provides ENOUGH up-to-date info for the "New Query".
            2. If yes, choose the best one.
            3. If the query implies needing LATEST real-time info and candidates are old, choose none.
            4. Return strictly JSON: {{"reuse": true/false, "index": <candidate_index_int>, "reason": "short explanation"}}
            """
            # åˆå§‹åŒ–æ¨¡å‹
            provider = os.getenv("LLM_PROVIDER", "ust")
            model_id = os.getenv("LLM_MODEL", "Qwen")
            host = os.getenv("LLM_HOST")
            if host:
                model = get_model(provider, model_id, host=host)
            else:
                model = get_model(provider, model_id)
                
            agent = Agent(model=model, markdown=True)
            
            response = agent.run(prompt)
            content = response.content
            
            # Parse JSON
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            elif '{' in content:
                 # Fallback for cases where LLM doesn't wrap in ```json
                 return json.loads(content[content.find('{'):content.rfind('}')+1])
            return {"reuse": False}
            
        except Exception as e:
            logger.warning(f"LLM evaluation failed: {e}")
            return {"reuse": False}

    def aggregate_search(self, query: str, engines: Optional[List[str]] = None, max_results: int = 5) -> str:
        """
        ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“åŒæ—¶æœç´¢å¹¶èšåˆç»“æœï¼Œè·å¾—æ›´å…¨é¢çš„ä¿¡æ¯è¦†ç›–ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯ã€‚
            engines: è¦ä½¿ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨ã€‚å¯é€‰å€¼: ["ddg", "baidu"]ã€‚
                     é»˜è®¤åŒæ—¶ä½¿ç”¨ ddg å’Œ baiduã€‚
            max_results: æ¯ä¸ªå¼•æ“æœŸæœ›è¿”å›çš„ç»“æœæ•°é‡ã€‚
        
        Returns:
            èšåˆåçš„æœç´¢ç»“æœï¼ŒæŒ‰å¼•æ“åˆ†ç»„æ˜¾ç¤ºã€‚
        """
        engines = engines or ["ddg", "baidu"]
        aggregated_results = []
        for engine in engines:
            res = self.search(query, engine=engine, max_results=max_results)
            aggregated_results.append(f"--- Results from {engine.upper()} ---\n{res}")
        
        return "\n\n".join(aggregated_results)
