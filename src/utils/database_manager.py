import sqlite3
import json
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional, Any, Union
import pandas as pd
from loguru import logger

class DatabaseManager:
    """
    SignalFlux æ•°æ®åº“ç®¡ç†å™¨ - è´Ÿè´£å­˜å‚¨çƒ­ç‚¹æ•°æ®ã€æœç´¢ç¼“å­˜å’Œè‚¡ä»·æ•°æ®
    ä½¿ç”¨ SQLite è¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨
    """
    
    def __init__(self, db_path: str = "data/signal_flux.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self._init_db()
        logger.info(f"ğŸ’¾ Database initialized at {self.db_path}")

    def _init_db(self):
        """åˆå§‹åŒ–è¡¨ç»“æ„"""
        cursor = self.conn.cursor()
        
        # 1. æ¯æ—¥çƒ­ç‚¹æ–°é—»è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS daily_news (
                id TEXT PRIMARY KEY,
                source TEXT,
                rank INTEGER,
                title TEXT,
                url TEXT,
                content TEXT,
                publish_time TEXT,
                crawl_time TEXT,
                sentiment_score REAL,
                analysis TEXT,
                meta_data TEXT
            )
        """)
        
        # å°è¯•æ·»åŠ  analysis åˆ—ï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†æ²¡æœ‰è¯¥åˆ—ï¼‰
        try:
            cursor.execute("ALTER TABLE daily_news ADD COLUMN analysis TEXT")
        except:
            pass  # åˆ—å·²å­˜åœ¨

        
        # 2. æœç´¢ç¼“å­˜è¡¨ (åŸæœ‰ JSON ç¼“å­˜)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS search_cache (
                query_hash TEXT PRIMARY KEY,
                query TEXT,
                engine TEXT,
                results TEXT,
                timestamp TEXT
            )
        """)

        # 2.5 æœç´¢è¯¦æƒ…è¡¨ (å±•å¼€çš„æœç´¢ç»“æœ)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS search_detail (
                id TEXT,
                query_hash TEXT,
                rank INTEGER,
                title TEXT,
                url TEXT,
                content TEXT,
                publish_time TEXT,
                crawl_time TEXT,
                sentiment_score REAL,
                source TEXT,
                meta_data TEXT,
                PRIMARY KEY (query_hash, id)
            )
        """)
        
        # 3. è‚¡ä»·æ•°æ®è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS stock_prices (
                ticker TEXT,
                date TEXT,
                open REAL,
                close REAL,
                high REAL,
                low REAL,
                volume REAL,
                change_pct REAL,
                PRIMARY KEY (ticker, date)
            )
        """)
        
        # 4. è‚¡ç¥¨åˆ—è¡¨è¡¨ (ç”¨äºæ£€ç´¢)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS stock_list (
                code TEXT PRIMARY KEY,
                name TEXT
            )
        """)
        
        # 5. æŠ•èµ„ä¿¡å·è¡¨ (ISQ Framework)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                title TEXT,
                summary TEXT,
                transmission_chain TEXT,
                sentiment_score REAL,
                confidence REAL,
                intensity INTEGER,
                expected_horizon TEXT,
                price_in_status TEXT,
                impact_tickers TEXT,
                industry_tags TEXT,
                sources TEXT,
                created_at TEXT
            )
        """)
        
        self.conn.commit()

    # --- æ–°é—»æ•°æ®æ“ä½œ ---
    
    def save_daily_news(self, news_list: List[Dict]) -> int:
        """ä¿å­˜çƒ­ç‚¹æ–°é—»ï¼ŒåŒ…å«å‘å¸ƒæ—¶é—´ä¸æŠ“å–æ—¶é—´"""
        cursor = self.conn.cursor()
        count = 0
        crawl_time = datetime.now().isoformat()
        
        for news in news_list:
            try:
                # å…¼å®¹ä¸åŒæ¥æºçš„ ID ç”Ÿæˆé€»è¾‘
                news_id = news.get('id') or f"{news.get('source')}_{news.get('rank')}_{crawl_time[:10]}"
                cursor.execute("""
                    INSERT OR REPLACE INTO daily_news 
                    (id, source, rank, title, url, content, publish_time, crawl_time, sentiment_score, meta_data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    news_id,
                    news.get('source'),
                    news.get('rank'),
                    news.get('title'),
                    news.get('url'),
                    news.get('content', ''),
                    news.get('publish_time'), # æ–°å¢æ”¯æŒå‘å¸ƒæ—¶é—´
                    crawl_time,
                    news.get('sentiment_score'),
                    json.dumps(news.get('meta_data', {}))
                ))
                count += 1
            except Exception as e:
                logger.error(f"Error saving news item {news.get('title')}: {e}")
        
        self.conn.commit()
        return count

    def get_daily_news(self, source: Optional[str] = None, limit: int = 100, days: int = 1) -> List[Dict]:
        """è·å–æœ€è¿‘ N å¤©çš„çƒ­ç‚¹æ–°é—»"""
        cursor = self.conn.cursor()
        # ä½¿ç”¨ crawl_time è¿‡æ»¤ï¼Œä¿è¯ç»“æœçš„æ–°é²œåº¦
        time_threshold = (datetime.now().timestamp() - days * 86400)
        time_threshold_str = datetime.fromtimestamp(time_threshold).isoformat()
        
        query = "SELECT * FROM daily_news WHERE crawl_time >= ?"
        params = [time_threshold_str]
        
        if source:
            query += " AND source = ?"
            params.append(source)
            
        query += " ORDER BY crawl_time DESC, rank LIMIT ?"
        params.append(limit)
        
        cursor.execute(query, params)
        return [dict(row) for row in cursor.fetchall()]

    def delete_news(self, news_id: str) -> bool:
        """åˆ é™¤ç‰¹å®šæ–°é—»"""
        cursor = self.conn.cursor()
        cursor.execute("DELETE FROM daily_news WHERE id = ?", (news_id,))
        self.conn.commit()
        return cursor.rowcount > 0
    
    def update_news_content(self, news_id: str, content: str = None, analysis: str = None) -> bool:
        """æ›´æ–°æ–°é—»çš„å†…å®¹æˆ–åˆ†æç»“æœ"""
        cursor = self.conn.cursor()
        updates = []
        params = []
        
        if content is not None:
            updates.append("content = ?")
            params.append(content)
        if analysis is not None:
            updates.append("analysis = ?")
            params.append(analysis)
            
        if not updates:
            return False
            
        params.append(news_id)
        query = f"UPDATE daily_news SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        self.conn.commit()
        return cursor.rowcount > 0

    # --- æœç´¢ç¼“å­˜è¾…åŠ© ---
    
    def get_search_cache(self, query_hash: str, ttl_seconds: Optional[int] = None) -> Optional[Dict]:
        """è·å–æœç´¢ç¼“å­˜ (ä¼˜å…ˆæŸ¥ search_detail)"""
        cursor = self.conn.cursor()
        
        # 1. å°è¯•ä» search_detail è·å–å±•å¼€çš„ç»“æ„åŒ–æ•°æ®
        cursor.execute("""
            SELECT * FROM search_detail 
            WHERE query_hash = ? 
            ORDER BY rank
        """, (query_hash,))
        details = [dict(row) for row in cursor.fetchall()]
        
        if details:
            # æ£€æŸ¥ TTL (å–ç¬¬ä¸€æ¡çš„æ—¶é—´)
            first_time = datetime.fromisoformat(details[0]['crawl_time'])
            if ttl_seconds and (datetime.now() - first_time).total_seconds() > ttl_seconds:
                logger.info(f"âŒ› Detailed cache expired for hash {query_hash}")
                pass # Expired, fall through or return None? If Detail expired, Cache likely expired too.
                # But let's check basic cache just in case metadata differs? 
                # Actually if details exist, we prefer them. If expired, we return None.
                return None
            
            logger.info(f"âœ… Hit detailed search cache for {query_hash} ({len(details)} items)")
            # Reconstruct the expected 'results' list format for SearchTools
            # SearchTools expects a list of dicts. 
            # We return a dict wrapper to match get_search_cache signature returning Dict usually containing 'results' string.
            # But SearchTools logic: 
            # cache = db.get_search_cache(...)
            # cached_data = json.loads(cache['results'])
            
            # To minimize SearchTools changes, we can return a dict mimicking the old structure
            # OR Change SearchTools to handle list return.
            # Let's return a special dict that SearchTools can recognize or just format it as before.
            return {"results": json.dumps(details), "timestamp": details[0]['crawl_time']}

        # 2. Fallback to old table
        cursor.execute("SELECT * FROM search_cache WHERE query_hash = ?", (query_hash,))
        row = cursor.fetchone()
        
        if not row:
            return None
            
        row_dict = dict(row)
        if ttl_seconds:
            cache_time = datetime.fromisoformat(row_dict['timestamp'])
            if (datetime.now() - cache_time).total_seconds() > ttl_seconds:
                logger.info(f"âŒ› Cache expired for hash {query_hash}")
                return None
                
        return row_dict

    def save_search_cache(self, query_hash: str, query: str, engine: str, results: Union[str, List[Dict]]):
        """ä¿å­˜æœç´¢ç»“æœ (åŒæ—¶ä¿å­˜åˆ° search_cache å’Œ search_detail)"""
        cursor = self.conn.cursor()
        current_time = datetime.now().isoformat()
        
        results_str = results if isinstance(results, str) else json.dumps(results)
        
        # 1. Save summary to search_cache
        cursor.execute("""
            INSERT OR REPLACE INTO search_cache (query_hash, query, engine, results, timestamp)
            VALUES (?, ?, ?, ?, ?)
        """, (query_hash, query, engine, results_str, current_time))
        
        # 2. Save details to search_detail if results is a list
        if isinstance(results, list):
            for item in results:
                try:
                    item_id = item.get('id') or f"{hash(item.get('url', ''))}"
                    cursor.execute("""
                        INSERT OR REPLACE INTO search_detail
                        (id, query_hash, rank, title, url, content, publish_time, crawl_time, sentiment_score, source, meta_data)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        str(item_id),
                        query_hash,
                        item.get('rank', 0),
                        item.get('title'),
                        item.get('url'),
                        item.get('content', ''),
                        item.get('publish_time'),
                        item.get('crawl_time') or current_time,
                        item.get('sentiment_score'),
                        item.get('source'),
                        json.dumps(item.get('meta_data', {}))
                    ))
                except Exception as e:
                    logger.error(f"Failed to save search detail {item.get('title')}: {e}")
                    
        self.conn.commit()

    def find_similar_queries(self, query: str, limit: int = 5) -> List[Dict]:
        """æ¨¡ç³Šæœç´¢ç›¸ä¼¼çš„å·²ç¼“å­˜æŸ¥è¯¢"""
        cursor = self.conn.cursor()
        
        # Simple fuzzy match: query in cached OR cached in query
        q_wild = f"%{query}%"
        cursor.execute("""
            SELECT query, query_hash, timestamp, results 
            FROM search_cache 
            WHERE query LIKE ? OR ? LIKE ('%' || query || '%')
            ORDER BY timestamp DESC
            LIMIT ?
        """, (q_wild, query, limit))
        
        return [dict(row) for row in cursor.fetchall()]

    def search_local_news(self, query: str, limit: int = 5) -> List[Dict]:
        """ä»æœ¬åœ° daily_news æœç´¢ç›¸å…³æ–°é—»"""
        cursor = self.conn.cursor()
        q_wild = f"%{query}%"
        # Search title and content
        cursor.execute("""
            SELECT * FROM daily_news
            WHERE title LIKE ? OR content LIKE ?
            ORDER BY crawl_time DESC
            LIMIT ?
        """, (q_wild, q_wild, limit))
        return [dict(row) for row in cursor.fetchall()]

    # --- è‚¡ç¥¨æ•°æ®æ“ä½œ ---

    def save_stock_list(self, df: pd.DataFrame):
        """ä¿å­˜è‚¡ç¥¨åˆ—è¡¨åˆ° stock_list è¡¨"""
        cursor = self.conn.cursor()
        try:
            # æ¸…ç©ºæ—§è¡¨
            cursor.execute("DELETE FROM stock_list")
            
            # æ‰¹é‡æ’å…¥
            data = df[['code', 'name']].to_dict('records')
            cursor.executemany(
                "INSERT INTO stock_list (code, name) VALUES (:code, :name)",
                data
            )
            self.conn.commit()
        except Exception as e:
            logger.error(f"Failed to save stock list: {e}")

    def search_stock(self, query: str, limit: int = 5) -> List[Dict]:
        """æ¨¡ç³Šæœç´¢è‚¡ç¥¨ä»£ç æˆ–åç§°"""
        cursor = self.conn.cursor()
        wild = f"%{query}%"
        cursor.execute("""
            SELECT code, name FROM stock_list 
            WHERE code LIKE ? OR name LIKE ? 
            LIMIT ?
        """, (wild, wild, limit))
        return [dict(row) for row in cursor.fetchall()]

    def save_stock_prices(self, ticker: str, df: pd.DataFrame):
        """ä¿å­˜è‚¡ä»·å†å²æ•°æ®"""
        if df.empty:
            return
            
        cursor = self.conn.cursor()
        
        # ç¡®ä¿ DataFrame æœ‰å¿…è¦çš„åˆ—
        required_cols = ['date', 'open', 'close', 'high', 'low', 'volume', 'change_pct']
        for col in required_cols:
            if col not in df.columns:
                logger.warning(f"Missing column {col} in stock data for {ticker}")
                return

        try:
            for _, row in df.iterrows():
                cursor.execute("""
                    INSERT OR REPLACE INTO stock_prices 
                    (ticker, date, open, close, high, low, volume, change_pct)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    ticker,
                    row['date'],
                    row['open'],
                    row['close'],
                    row['high'],
                    row['low'],
                    row['volume'],
                    row['change_pct']
                ))
            self.conn.commit()
        except Exception as e:
            logger.error(f"Failed to save stock prices for {ticker}: {e}")

    def get_stock_prices(self, ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„è‚¡ä»·æ•°æ®"""
        cursor = self.conn.cursor()
        
        cursor.execute("""
            SELECT * FROM stock_prices 
            WHERE ticker = ? AND date >= ? AND date <= ?
            ORDER BY date
        """, (ticker, start_date, end_date))
        
        rows = cursor.fetchall()
        if not rows:
            return pd.DataFrame()
            
        columns = ['ticker', 'date', 'open', 'close', 'high', 'low', 'volume', 'change_pct']
        return pd.DataFrame([dict(row) for row in rows], columns=columns)

    def execute_query(self, query: str, params: tuple = ()) -> List[Any]:
        """æ‰§è¡Œè‡ªå®šä¹‰ SQL æŸ¥è¯¢"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(query, params)
            if query.strip().upper().startswith("SELECT"):
                return cursor.fetchall()
            else:
                self.conn.commit()
                return []
        except Exception as e:
            logger.error(f"SQL execution failed: {e}")
            return []

    # --- æŠ•èµ„ä¿¡å·æ“ä½œ (ISQ Framework) ---

    def save_signal(self, signal: Dict[str, Any]):
        """ä¿å­˜æŠ•èµ„ä¿¡å·"""
        cursor = self.conn.cursor()
        created_at = datetime.now().isoformat()
        
        cursor.execute("""
            INSERT OR REPLACE INTO signals 
            (signal_id, title, summary, transmission_chain, sentiment_score, 
             confidence, intensity, expected_horizon, price_in_status, 
             impact_tickers, industry_tags, sources, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            signal.get('signal_id'),
            signal.get('title'),
            signal.get('summary'),
            json.dumps(signal.get('transmission_chain', [])),
            signal.get('sentiment_score', 0.0),
            signal.get('confidence', 0.0),
            signal.get('intensity', 1),
            signal.get('expected_horizon', 'T+0'),
            signal.get('price_in_status', 'æœªçŸ¥'),
            json.dumps(signal.get('impact_tickers', [])),
            json.dumps(signal.get('industry_tags', [])),
            json.dumps(signal.get('sources', [])),
            created_at
        ))
        self.conn.commit()

    def get_recent_signals(self, limit: int = 20) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æŠ•èµ„ä¿¡å·"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT * FROM signals ORDER BY created_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        signals = []
        for row in rows:
            d = dict(row)
            # è§£æ JSON å­—æ®µ
            for field in ['transmission_chain', 'impact_tickers', 'industry_tags', 'sources']:
                if d.get(field):
                    try:
                        d[field] = json.loads(d[field])
                    except:
                        pass
            signals.append(d)
        return signals

    def close(self):
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed.")
