import json
from datetime import datetime, timedelta
import pandas as pd
from typing import List, Dict, Any
from agno.agent import Agent
from agno.models.base import Model
from loguru import logger

from utils.database_manager import DatabaseManager
from utils.hybrid_search import InMemoryRAG
from utils.json_utils import extract_json
from utils.stock_tools import StockTools
import re
from schema.models import InvestmentSignal, InvestmentReport, TransmissionNode
from prompts.report_agent import (
    get_cluster_planner_instructions,
    get_report_planner_instructions,
    get_report_writer_instructions,
    get_report_editor_instructions,
    get_section_editor_instructions,
    get_summary_generator_instructions,
    get_final_assembly_instructions
)


class ReportAgent:
    """
    ç ”æŠ¥ç”Ÿæˆå™¨ (ReportAgent) - Map-Reduce æ¶æ„
    æ”¯æŒå¢é‡ç¼–è¾‘æ¨¡å¼ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ç« èŠ‚
    """
    
    def __init__(self, db: DatabaseManager, model: Model, incremental_edit: bool = True):
        self.db = db
        self.model = model
        self.incremental_edit = incremental_edit
        
        # 0. InMemory RAG for cross-chapter context
        self.rag = InMemoryRAG(data=[], text_fields=["title", "content", "summary"])
        
        # 1. Planner Agent
        self.planner = Agent(
            model=model,
            tools=[self.rag.search],
            markdown=True,
            debug_mode=True
        )
        
        # 2. Writer Agent
        self.writer = Agent(
            model=model,
            markdown=True,
            debug_mode=True
        )
        
        # 3. Editor Agent
        self.editor = Agent(
            model=model,
            tools=[self.rag.search],
            markdown=True,
            debug_mode=True
        )
        
        # 5. Section Editor Agent (ç”¨äºå¢é‡ç¼–è¾‘)
        self.section_editor = Agent(
            model=model,
            tools=[self.rag.search],
            markdown=True,
            debug_mode=True
        )
        
        logger.info(f"ğŸ“ ReportAgent initialized (incremental_edit={incremental_edit})")

    def _format_signal_input(self, signal: Any, index: int) -> str:
        """æ ¼å¼åŒ–ä¿¡å·ä¾› prompt ä½¿ç”¨ (é€‚é… InvestmentSignal æ¨¡å‹)"""
        # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬ä¸ºæ¨¡å‹
        if isinstance(signal, dict):
            try:
                sig_obj = InvestmentSignal(**signal)
            except:
                # Fallback for old dicts
                return f"--- ä¿¡å· [{index}] ---\næ ‡æ ¼: {signal.get('title')}\nå†…å®¹: {signal.get('content', '')[:500]}"
        else:
            sig_obj = signal

        chain_str = " -> ".join([f"{n.node_name}({n.impact_type})" for n in sig_obj.transmission_chain])
        
        text = f"--- ä¿¡å· [{index}] ---\n"
        text += f"æ ‡é¢˜: {sig_obj.title}\n"
        text += f"é€»è¾‘æ‘˜è¦: {sig_obj.summary}\n"
        text += f"ä¼ å¯¼é“¾æ¡: {chain_str}\n"
        text += f"ISQ è¯„åˆ†: æƒ…ç»ª({sig_obj.sentiment_score}), ç¡®å®šæ€§({sig_obj.confidence}), å¼ºåº¦({sig_obj.intensity})\n"
        text += f"é¢„æœŸåšå¼ˆ: æ—¶çª—({sig_obj.expected_horizon}), é¢„æœŸå·®({sig_obj.price_in_status})\n"
        
        tickers = ", ".join([f"{t.get('name')}({t.get('ticker')})" for t in sig_obj.impact_tickers])
        if tickers:
            text += f"å—å½±å“æ ‡çš„: {tickers}\n"
            
        return text

    def _cluster_signals(self, signals: List[Dict[str, Any]], user_query: str = None) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Planner å°†ä¿¡å·èšç±»ä¸ºå‡ ä¸ªæ ¸å¿ƒä¸»é¢˜
        è¿”å›: [{"theme_title": "ä¸»é¢˜A", "signal_ids": [1, 2], "rationale": "..."}]
        """
        # å‡†å¤‡ç®€è¦è¾“å…¥
        signals_preview = ""
        for i, s in enumerate(signals, 1):
            title = s.title if hasattr(s, 'title') else s.get('title', '')
            signals_preview += f"[{i}] {title}\n"
            
        logger.info(f"ğŸ§  Clustering {len(signals)} signals into themes...")
        
        instruction = get_cluster_planner_instructions(signals_preview, user_query)
        self.planner.instructions = [instruction]
        
        try:
            response = self.planner.run("è¯·å¯¹ä»¥ä¸Šä¿¡å·è¿›è¡Œä¸»é¢˜èšç±»ã€‚")
            content = response.content
            
            cluster_data = extract_json(content)
            if cluster_data and "clusters" in cluster_data:
                clusters = cluster_data["clusters"]
                logger.info(f"âœ… Created {len(clusters)} signal clusters.")
                return clusters
            else:
                logger.warning("âš ï¸ Failed to parse cluster JSON, fallback to individual signal mode.")
                return []
                
        except Exception as e:
            logger.error(f"Signal clustering failed: {e}")
            return []

    def generate_report(self, signals: List[Dict[str, Any]], user_query: str = None) -> str:
        """
        æ‰§è¡Œ Write-Plan-Edit æµç¨‹ç”Ÿæˆç ”æŠ¥
        """
        stock_tools = StockTools(self.db, auto_update=False)

        logger.info(f"ğŸ“ Starting report generation for {len(signals)} signals...")
        
        # --- Phase 1: Signal Clustering ---
        clusters = self._cluster_signals(signals, user_query)
        
        # å¦‚æœèšç±»å¤±è´¥ï¼Œæˆ–è€…æ²¡æœ‰è¿”å› clustersï¼Œåˆ™å›é€€åˆ°æ¯ä¸ªä¿¡å·ä¸€èŠ‚ï¼ˆæ¨¡æ‹Ÿæ¯ä¸ªä¿¡å·æ˜¯ä¸€ä¸ªç°‡ï¼‰
        if not clusters:
             clusters = [{"theme_title": (s.title if hasattr(s, 'title') else s.get('title', '')), "signal_ids": [i]} for i, s in enumerate(signals, 1)]

        # --- Phase 2: Writing Drafts based on Clusters ---
        sections = []
        sources_list_lines = []
        section_titles = []  # å­˜å‚¨ (anchor, title)
        
        for i, cluster in enumerate(clusters, 1):
            theme_title = cluster.get("theme_title", f"ä¸»é¢˜ {i}")
            signal_ids = cluster.get("signal_ids", [])
            rationale = cluster.get("rationale", "")
            
            logger.info(f"âœï¸ Writing draft for theme [{i}/{len(clusters)}]: {theme_title} (Signals: {signal_ids})...")
            
            # èšåˆè¯¥ç°‡ä¸‹çš„æ‰€æœ‰ä¿¡å·å†…å®¹
            cluster_signals_text = ""
            cluster_price_context = ""
            cluster_tickers_seen = set()
            
            for sig_idx in signal_ids:
                # æ³¨æ„ï¼šsignal_ids æ˜¯ 1-basedï¼Œè®¿é—® list éœ€è¦ -1
                if sig_idx < 1 or sig_idx > len(signals):
                    continue
                    
                signal = signals[sig_idx-1]
                
                # æ”¶é›† Sources
                if hasattr(signal, 'sources'):
                    for src in signal.sources:
                        sources_list_lines.append(f"[{sig_idx}] {src.get('title')} ({src.get('source_name')}), {src.get('url', 'N/A')}")
                elif isinstance(signal, dict) and 'source' in signal:
                    sources_list_lines.append(f"[{sig_idx}] {signal.get('title')} ({signal.get('source')}), {signal.get('url', 'N/A')}")
                
                # èšåˆä¿¡å·æ–‡æœ¬
                cluster_signals_text += self._format_signal_input(signal, sig_idx) + "\n"
                
                # èšåˆè¡Œæƒ… Context (å»é‡)
                analysis_text = getattr(signal, 'analysis', '') if not isinstance(signal, dict) else signal.get('analysis', '')
                potential_tickers = list(set(re.findall(r'\b(\d{6})\b', analysis_text)))
                for t in potential_tickers:
                    if t not in cluster_tickers_seen:
                        cluster_tickers_seen.add(t)
                        # è·å–è¡Œæƒ…
                        try:
                            end_date = datetime.now().strftime("%Y-%m-%d")
                            start_date = (datetime.now() - timedelta(days=15)).strftime("%Y-%m-%d")
                            df_ctx = stock_tools.get_stock_price(t, start_date=start_date, end_date=end_date)
                            if not df_ctx.empty:
                                last_5 = df_ctx.tail(5)
                                prices_str = ", ".join([f"{row['date']}:{row['close']}" for _, row in last_5.iterrows()])
                                cluster_price_context += f"- {t}: {prices_str}\n"
                        except:
                            continue

            # æ’°å†™å•èŠ‚è‰ç¨¿ (åŸºäºä¸»é¢˜)
            writer_instruction = get_report_writer_instructions(
                theme_title=theme_title,
                signal_cluster_text=cluster_signals_text,
                signal_indices=signal_ids,
                price_context=cluster_price_context,
                user_query=user_query
            )
            
            try:
                self.writer.instructions = [writer_instruction] 
                response = self.writer.run(f"è¯·ä¾æ®ä¸»é¢˜ '{theme_title}' å’Œ è¾“å…¥ä¿¡å·é›† å¼€å§‹æ’°å†™ã€‚")
                content = response.content.strip()
                
                # å°è¯•æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                lines = content.split('\n')
                title_line = lines[0].strip().replace('###', '').strip().replace('#', '')
                # å¦‚æœç¬¬ä¸€è¡Œå¤ªé•¿æˆ–è€…æ²¡æ ‡é¢˜ï¼Œå°±ç”¨ theme_title
                final_title = title_line if title_line and len(title_line) < 50 else theme_title
                
                # å­˜å‚¨åŸå§‹ç« èŠ‚ï¼Œå¸¦é”šç‚¹
                section_content = f"<a id=\"section-{i}\"></a>\n\n{content}\n"
                sections.append(section_content)
                section_titles.append((f"section-{i}", final_title))
                
            except Exception as e:
                logger.error(f"Failed to write section for theme {theme_title}: {e}")
        
        if not sections:
            return "âš ï¸ æ— æ³•ç”Ÿæˆç ”æŠ¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç« èŠ‚ã€‚"

        sources_list_text = "\n".join(sources_list_lines)
        
        # --- Decision Point: Incremental vs Global ---
        # å¦‚æœå¼€å¯å¢é‡ç¼–è¾‘ï¼Œæˆ–è€…å†…å®¹æ€»é•¿åº¦è¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚ 80000 å­—ç¬¦ï¼‰ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼ä»¥é¿å…ä¸Šä¸‹æ–‡æº¢å‡º
        total_content_length = sum(len(s) for s in sections)
        use_incremental = self.incremental_edit or total_content_length > 80000
        
        if use_incremental:
            logger.info(f"ğŸ”„ Using INCREMENTAL editing mode (sections={len(sections)})...")
            final_response_content = self._incremental_edit(sections, sources_list_text, section_titles)
        else:
            # --- Phase 3: Global Planning (The Planner) ---
            # è™½ç„¶å·²ç»èšç±»ï¼Œä½†å…¨å±€ Planner ä»æœ‰åŠ©äºè°ƒæ•´ç« èŠ‚é¡ºåºå’Œè¯†åˆ«åˆ†æ­§
            logger.info("ğŸ§  Using GLOBAL Planning & Editing mode...")
            
            # ... (Rest of global logic remains mostly the same, just operating on theme sections)
            draft_docs = []
            toc_lines = []
            for i, section in enumerate(sections, 1):
                title = section_titles[i-1][1]
                draft_docs.append({
                    "id": str(i),
                    "title": title,
                    "content": section,
                    "summary": section[:500]
                })
                toc_lines.append(f"[{i}] {title}")
            
            self.rag.update_data(draft_docs)
            toc_text = "\n".join(toc_lines)
            
            planner_instruction = get_report_planner_instructions(toc_text, len(signals), user_query)
            self.planner.instructions = [planner_instruction]
            
            try:
                plan_response = self.planner.run("è¯·é˜…è¯»ç°æœ‰è‰ç¨¿å¹¶è§„åˆ’ç»ˆç¨¿å¤§çº²ã€‚")
                report_plan = plan_response.content
                logger.info("âœ… Report plan generated.")
            except Exception as e:
                logger.error(f"Planning failed: {e}")
                report_plan = "ï¼ˆè§„åˆ’å¤±è´¥ï¼Œè¯·æŒ‰é»˜è®¤é¡ºåºç¼–æ’ï¼‰"

            # --- Phase 4: Final Editing (The Editor) ---
            logger.info("ğŸ¬ Editing final report based on plan...")
            
            all_drafts_text = "\n---\n".join(sections)
            editor_instruction = get_report_editor_instructions(all_drafts_text, report_plan, sources_list_text)
            self.editor.instructions = [editor_instruction]
            
            try:
                # ä½¿ç”¨ Editor è¿›è¡Œé‡ç»„å’Œæ¶¦è‰²
                final_response = self.editor.run("è¯·æ ¹æ®è§„åˆ’å¤§çº²å’Œè‰ç¨¿å†…å®¹ï¼Œç”Ÿæˆæœ€ç»ˆç ”æŠ¥ã€‚")
                final_response_content = final_response.content
            except Exception as e:
                logger.error(f"Final editing failed: {e}")
                final_response_content = f"# ç ”æŠ¥ç”Ÿæˆå¤±è´¥\n\n{e}"

        # æ¸…ç† Markdown æ ‡è®°
        final_response_content = final_response_content.strip()
        if final_response_content.startswith("```markdown"):
            final_response_content = final_response_content[len("```markdown"):].strip()
        if final_response_content.startswith("```"):
            final_response_content = final_response_content[3:].strip()
        if final_response_content.endswith("```"):
            final_response_content = final_response_content[:-3].strip()

        # ç»Ÿä¸€æ·»åŠ  TOC (å¦‚æœ Editor æœªç”Ÿæˆ)
        if not use_incremental and "[TOC]" not in final_response_content:
             lines = final_response_content.split('\n')
             if lines and lines[0].strip().startswith('# '):
                 # æ’å…¥åœ¨æ ‡é¢˜ä¹‹å
                 final_response_content = lines[0] + "\n\n[TOC]\n\n" + "\n".join(lines[1:])
             else:
                 # æ’å…¥åœ¨æœ€å‰
                 final_response_content = "[TOC]\n\n" + final_response_content
        
        # Fix duplicate headers (e.g. "#### #### Title") caused by LLM stutter
        final_response_content = re.sub(r'(#{1,6})\s+\1', r'\1', final_response_content)
        
        # --- Phase 5: Visualization Processing ---
        logger.info("ğŸ¨ Processing visualization...")
        final_report_with_charts = self._process_charts(final_response_content)
        
        return final_report_with_charts

    def _clean_markdown(self, text: str) -> str:
        """Helper to remove markdown code fences"""
        text = text.strip()
        if text.startswith("```markdown"):
            text = text[len("```markdown"):].strip()
        elif text.startswith("```"):
            text = text[3:].strip()
        if text.endswith("```"):
            text = text[:-3].strip()
        return text

    def _incremental_edit(self, sections: List[str], sources_list_text: str, section_titles_data: List[tuple] = None) -> str:
        """å¢é‡ç¼–è¾‘æ¨¡å¼"""
        import time
        
        # 1. å¡«å…… RAG
        draft_docs = []
        toc_lines = []
        for i, section in enumerate(sections, 1):
            if section_titles_data and i <= len(section_titles_data):
                _, title = section_titles_data[i-1]
            else:
                title = f"ç« èŠ‚ {i}"
            
            draft_docs.append({
                "id": str(i),
                "title": title,
                "content": section,
                "summary": section[:300]
            })
            toc_lines.append(f"[{i}] {title}")
        
        self.rag.update_data(draft_docs)
        toc = "\n".join(toc_lines)
        
        # 2. é€èŠ‚ç¼–è¾‘
        edited_sections = []
        for i, section in enumerate(sections, 1):
            logger.info(f"âœï¸ Incremental editing: section {i}/{len(sections)}...")
            
            editor_instruction = get_section_editor_instructions(i, len(sections), toc)
            self.section_editor.instructions = [editor_instruction]
            
            try:
                response = self.section_editor.run(f"è¯·ç¼–è¾‘ä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼š\n\n{section}")
                cleaned_content = self._clean_markdown(response.content)
                edited_sections.append(cleaned_content)
            except Exception as e:
                logger.warning(f"âš ï¸ Section {i} editing failed: {e}, using original")
                edited_sections.append(self._clean_markdown(section))
            
            # ç®€çŸ­å»¶è¿Ÿé¿å… API è¿‡è½½
            time.sleep(0.5)
        
        # 3. ç”Ÿæˆæ‘˜è¦
        logger.info("ğŸ“ Generating summary (incremental)...")
        section_summaries = "\n".join([s[:200] + "..." for s in edited_sections])
        summary_instruction = get_summary_generator_instructions(toc, section_summaries)
        self.editor.instructions = [summary_instruction]
        
        try:
            summary_response = self.editor.run("è¯·ç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹æ‘˜è¦ã€‚")
            summary = self._clean_markdown(summary_response.content)
        except Exception as e:
            logger.warning(f"âš ï¸ Summary generation failed: {e}")
            summary = "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè¯·å‚é˜…å„ç« èŠ‚è¯¦æƒ…ã€‚ï¼‰"
        
        # 4. ç”Ÿæˆå‚è€ƒæ–‡çŒ®å’Œå°¾éƒ¨å†…å®¹
        logger.info("ğŸ“š Generating references (incremental)...")
        assembly_instruction = get_final_assembly_instructions(sources_list_text)
        self.editor.instructions = [assembly_instruction]
        
        try:
            tail_response = self.editor.run("è¯·ç”Ÿæˆå‚è€ƒæ–‡çŒ®ã€é£é™©æç¤ºå’Œå¿«é€Ÿæ‰«æè¡¨æ ¼ã€‚")
            tail_content = self._clean_markdown(tail_response.content)
            
            # åˆ†ç¦»å¿«é€Ÿæ‰«æå’Œå…¶ä»–å°¾éƒ¨å†…å®¹
            quick_scan = ""
            other_tail = tail_content
            if "å¿«é€Ÿæ‰«æ" in tail_content:
                parts = tail_content.split("## å¿«é€Ÿæ‰«æ")
                if len(parts) == 2:
                    other_tail = parts[0].strip()
                    quick_scan = "## å¿«é€Ÿæ‰«æ" + parts[1].split("## ")[0] if "## " in parts[1] else "## å¿«é€Ÿæ‰«æ" + parts[1]
        except Exception as e:
            logger.warning(f"âš ï¸ Tail content generation failed: {e}")
            quick_scan = ""
            other_tail = f"""## å‚è€ƒæ–‡çŒ®

            {sources_list_text}

            ## é£é™©æç¤º

            æœ¬æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚
            """
        
        # 5. ç»„è£…æœ€ç»ˆæŠ¥å‘Š
        current_date = datetime.now().strftime('%Y-%m-%d')
        
        import textwrap
        import re
        
        # æ¸…ç† edited_sectionsï¼šåªåšä»£ç å—ä¿æŠ¤å’ŒåŸºæœ¬æ¸…ç†
        
        # æ¸…ç† edited_sections ä¸­çš„æ ‡é¢˜å±‚çº§é—®é¢˜
        cleaned_sections = []
        for section in edited_sections:
            # ä¿æŠ¤ä»£ç å—ï¼šå…ˆä¸´æ—¶æ›¿æ¢ä»£ç å—å†…å®¹
            code_blocks = []
            def preserve_code_block(match):
                code_blocks.append(match.group(0))
                return f"__CODE_BLOCK_{len(code_blocks) - 1}__"
            
            section_protected = re.sub(r'```[\s\S]*?```', preserve_code_block, section)
            
            # åªæ¸…ç†æ˜æ˜¾çš„é”™è¯¯ï¼šé‡å¤çš„ # ç¬¦å·ï¼ˆLLM stutterï¼‰
            # ç§»é™¤é‡å¤çš„ # ç¬¦å·
            section_fixed = re.sub(r'(#{1,6})\s+\1+', r'\1', section_protected)
            
            # æ¢å¤ä»£ç å—
            for i, block in enumerate(code_blocks):
                section_fixed = section_fixed.replace(f"__CODE_BLOCK_{i}__", block)
            
            cleaned_sections.append(section_fixed)
        
        # Use simple string concatenation or 0-indented string to avoid dedent issues with dynamic content
        final_report = f"""# SignalFlux å…¨çƒå¸‚åœºè¶‹åŠ¿æ—¥æŠ¥ ({current_date})

[TOC]

{quick_scan}

## æ ¸å¿ƒè§‚ç‚¹æ‘˜è¦

{summary}

{"\n\n".join(cleaned_sections)}

{other_tail}
"""
        # Fix duplicate headers (e.g. "#### #### Title") caused by LLM stutter
        final_report = re.sub(r'(#{1,6})\s+\1', r'\1', final_report)
        
        # ç§»é™¤è¿ç»­çš„ç©ºè¡Œï¼ˆæœ€å¤šä¿ç•™2ä¸ªï¼‰
        final_report = re.sub(r'\n{4,}', '\n\n\n', final_report)
         
        return final_report.strip()
    

    def _process_charts(self, content: str) -> str:
        """è§£æ json-chart ä»£ç å—å¹¶æ›¿æ¢ä¸º HTML é“¾æ¥/Iframe"""

        import re
        from utils.visualizer import VisualizerTools
        from utils.stock_tools import StockTools
        
        stock_tools = StockTools(self.db, auto_update=False)

        def replace_match(match):
            from utils.json_utils import extract_json
            json_str = match.group(1).strip()
            try:
                config = extract_json(json_str)
                if not config:
                    raise ValueError("No valid JSON found in chart block")
                
                chart_type = config.get("type")
                
                if chart_type == "stock":
                    ticker_raw = config.get("ticker", "")
                    base_title = config.get("title", f"{ticker_raw} èµ°åŠ¿")
                    prediction = config.get("prediction", None)
                    
                    # å¤„ç†å¤šä¸ª ticker çš„æƒ…å†µï¼ˆé€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼‰
                    tickers = re.split(r'[,\s]+', str(ticker_raw).strip())
                    
                    # å°è¯•è§£ææ¯ä¸ª ticker
                    valid_tickers = []
                    for t in tickers:
                        t = t.strip()
                        if not t:
                            continue
                        
                        # æ ‡å‡†6ä½æ•°å­—æ ¼å¼
                        if len(t) == 6 and t.isdigit():
                            valid_tickers.append(t)
                        # å¸¦åç¼€æ ¼å¼ï¼š301367.SZ, 600519.SH
                        elif '.' in t:
                            code_part = t.split('.')[0]
                            if len(code_part) == 6 and code_part.isdigit():
                                valid_tickers.append(code_part)
                                logger.info(f"ğŸ“Š Extracted ticker {code_part} from {t}")
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆç”¨å…¬å¸åæœç´¢ï¼‰
                        elif len(t) > 1:
                            try:
                                search_results = stock_tools.search_ticker(t)
                                if search_results and len(search_results) > 0:
                                    first_match = search_results[0].get('code', '')
                                    if first_match:
                                        valid_tickers.append(first_match)
                                        logger.info(f"ğŸ“Š Fuzzy matched ticker {first_match} from query '{t}'")
                            except Exception as e:
                                logger.warning(f"âš ï¸ Fuzzy search failed for {t}: {e}")
                    
                    tickers = valid_tickers
                    
                    if not tickers:
                        logger.warning(f"âš ï¸ No valid ticker found in: {ticker_raw}")
                        return f"\n<!-- æ— æ³•è§£æè‚¡ç¥¨ä»£ç : {ticker_raw} -->\n"

                    
                    if len(tickers) > 1:
                        logger.info(f"ğŸ“Š Multiple tickers detected: {tickers}, generating charts for all")
                    
                    # ä¸ºæ¯ä¸ª ticker ç”Ÿæˆå›¾è¡¨
                    all_charts_html = []
                    end_date = datetime.now().strftime("%Y-%m-%d")
                    start_date = (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d")
                    
                    for idx, ticker in enumerate(tickers):
                        # å¦‚æœæœ‰å¤šä¸ª tickerï¼Œä¸ºæ¯ä¸ªç”Ÿæˆç‹¬ç«‹çš„æ ‡é¢˜
                        if len(tickers) > 1:
                            chart_title = f"{ticker} - {base_title}"
                        else:
                            chart_title = base_title
                        
                        df = stock_tools.get_stock_price(ticker, start_date=start_date, end_date=end_date)
                        
                        if not df.empty:
                            # å¦‚æœæœ‰ prediction ä¸”æ˜¯å¤šä¸ª tickerï¼Œå°è¯•åˆ†é…é¢„æµ‹å€¼
                            ticker_prediction = None
                            if prediction and isinstance(prediction, list):
                                # å‡è®¾é¢„æµ‹å€¼å¹³å‡åˆ†é…ç»™æ¯ä¸ª ticker
                                chunk_size = len(prediction) // len(tickers) if len(tickers) > 1 else len(prediction)
                                if chunk_size > 0:
                                    start_idx = idx * chunk_size
                                    end_idx = start_idx + chunk_size
                                    ticker_prediction = prediction[start_idx:end_idx] if end_idx <= len(prediction) else prediction[start_idx:]
                                if not ticker_prediction:
                                    ticker_prediction = prediction[:3] if len(prediction) >= 3 else prediction
                            
                            chart = VisualizerTools.generate_stock_chart(df, ticker, chart_title, ticker_prediction)
                            if chart:
                                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                                filename = f"reports/charts/{ticker}_{timestamp}.html"
                                VisualizerTools.render_chart_to_file(chart, filename)
                                
                                rel_path = f"charts/{ticker}_{timestamp}.html"
                                all_charts_html.append(
                                    f'<iframe src="{rel_path}" width="100%" height="500px" style="border:none;"></iframe>\n'
                                    f'<p style="text-align:center;color:gray;font-size:12px">äº¤äº’å¼å›¾è¡¨: {chart_title}</p>'
                                )
                        else:
                            logger.warning(f"âš ï¸ No data for ticker: {ticker}")
                    
                    if all_charts_html:
                        return "\n" + "\n".join(all_charts_html) + "\n"
                    else:
                        return f"\n<!-- æ— æ³•è·å–è‚¡ç¥¨æ•°æ®: {ticker_raw} -->\n"


                
                elif chart_type == "sentiment":
                    keywords = config.get("keywords", [])
                    title = config.get("title", "èˆ†æƒ…æƒ…ç»ªè¶‹åŠ¿")
                    
                    if keywords:
                        # ç®€å•çš„ SQL æŸ¥è¯¢ (æ³¨æ„å¯èƒ½æœ‰ SQL æ³¨å…¥é£é™©ï¼Œä½†åœ¨ Agent å†…éƒ¨å¯æ§)
                        # æ„é€  OR æŸ¥è¯¢ä»¥è·å–æ›´å¤šç›¸å…³æ•°æ®
                        conditions = " OR ".join([f"content LIKE '%{k}%'" for k in keywords])
                        query = f"SELECT publish_time, sentiment_score FROM daily_news WHERE ({conditions}) AND sentiment_score IS NOT NULL ORDER BY publish_time"
                        
                        logger.info(f"ğŸ“Š Executing sentiment query: {query}")
                        results = self.db.execute_query(query)
                        logger.info(f"ğŸ“Š Query result count: {len(results)}")
                        
                        if not results or len(results) == 0:
                            # Fallback: Try broadening search by splitting keywords
                            logger.info("âš ï¸ Initial sentiment query empty, attempting fallback with split keywords...")
                            broad_keywords = []
                            for k in keywords:
                                broad_keywords.extend(k.split())
                            
                            # Deduplicate and filter short words
                            broad_keywords = list(set([k for k in broad_keywords if len(k) > 1]))
                            
                            if broad_keywords:
                                conditions = " OR ".join([f"content LIKE '%{k}%'" for k in broad_keywords])
                                query = f"SELECT publish_time, sentiment_score FROM daily_news WHERE ({conditions}) AND sentiment_score IS NOT NULL ORDER BY publish_time"
                                logger.info(f"ğŸ“Š Executing fallback sentiment query: {query}")
                                results = self.db.execute_query(query)
                                logger.info(f"ğŸ“Š Fallback query result count: {len(results)}")

                        if results:
                            # æ ¼å¼åŒ–æ•°æ®
                            sentiment_history = []
                            for row in results:
                                try:
                                    # å‡è®¾ publish_time æ˜¯å­—ç¬¦ä¸²ï¼Œæˆ–è€… date object
                                    dt = row[0]
                                    if isinstance(dt, datetime):
                                        d_str = dt.strftime("%Y-%m-%d")
                                    else:
                                        d_str = str(dt)[:10] # æˆªå–æ—¥æœŸéƒ¨åˆ†
                                        
                                    sentiment_history.append({"date": d_str, "score": row[1]})
                                except:
                                    continue
                            
                            # èšåˆæ¯å¤©çš„å¹³å‡åˆ†
                            df_sent = pd.DataFrame(sentiment_history)
                            if not df_sent.empty:
                                df_sent = df_sent.groupby('date')['score'].mean().reset_index()
                                sentiment_history_agg = df_sent.to_dict('records')
                                
                                chart = VisualizerTools.generate_sentiment_trend_chart(sentiment_history_agg)
                                if chart:
                                    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                                    filename = f"reports/charts/sentiment_{timestamp}.html"
                                    VisualizerTools.render_chart_to_file(chart, filename)
                                    rel_path = f"charts/sentiment_{timestamp}.html"
                                    return f'\n<iframe src="{rel_path}" width="100%" height="400px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">äº¤äº’å¼å›¾è¡¨: {title}</p>\n'
                        
                        # Fallback for sentiment if query results are empty
                        return f'\n<p style="text-align:center;color:gray;font-size:12px;padding:20px;border:1px dashed #ccc;border-radius:8px;">ğŸ“Š æš‚æ— è¶³å¤Ÿå†å²æ•°æ®ç”Ÿæˆ "{title}" çš„è¶‹åŠ¿å›¾</p>\n'

                elif chart_type == "isq":
                    sentiment = config.get("sentiment", 0.0)
                    confidence = config.get("confidence", 0.5)
                    intensity = config.get("intensity", 3)
                    expectation_gap = config.get("expectation_gap", 0.5)
                    timeliness = config.get("timeliness", 0.8)
                    title = config.get("title", "ä¿¡å·è´¨é‡ ISQ è¯„ä¼°")
                    
                    chart = VisualizerTools.generate_isq_radar_chart(
                        sentiment, confidence, intensity, 
                        expectation_gap=expectation_gap, 
                        timeliness=timeliness, 
                        title=title
                    )
                    if chart:
                        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                        filename = f"reports/charts/isq_{timestamp}.html"
                        VisualizerTools.render_chart_to_file(chart, filename)
                        rel_path = f"charts/isq_{timestamp}.html"
                        return f'\n<iframe src="{rel_path}" width="100%" height="420px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">ä¿¡å·è´¨é‡é›·è¾¾å›¾: {title}</p>\n'

                elif chart_type == "transmission":
                    nodes = config.get("nodes", [])
                    title = config.get("title", "æŠ•èµ„é€»è¾‘ä¼ å¯¼é“¾æ¡")
                    
                    if nodes:
                        # ç”ŸæˆåŸºäºèŠ‚ç‚¹å†…å®¹çš„å”¯ä¸€æ ‡è¯†ï¼Œé¿å…ç›¸åŒæ—¶é—´æˆ³ä¸‹çš„é‡å¤å›¾è¡¨
                        import hashlib
                        nodes_str = json.dumps(nodes, sort_keys=True, ensure_ascii=False)
                        content_hash = hashlib.md5(nodes_str.encode()).hexdigest()[:8]
                        
                        chart = VisualizerTools.generate_transmission_graph(nodes, title)
                        if chart:
                            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                            filename = f"reports/charts/trans_{timestamp}_{content_hash}.html"
                            VisualizerTools.render_chart_to_file(chart, filename)
                            rel_path = f"charts/trans_{timestamp}_{content_hash}.html"
                            return f'\n<iframe src="{rel_path}" width="100%" height="420px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">é€»è¾‘ä¼ å¯¼æ‹“æ‰‘å›¾: {title}</p>\n'

                # å¦‚æœæ˜¯å…¶ä»–ç±»å‹æˆ–å¤±è´¥ï¼Œä¿ç•™åŸæ–‡æˆ–è€…æ˜¾ç¤ºé”™è¯¯
                return f"```json\n{json_str}\n```" # Fallback to json display if render fails logic mismatch
            
            except Exception as e:
                logger.error(f"Chart processing failed: {e}")
                return match.group(0) # Return original text on error

        # åŒ¹é… ```json-chart ... ```
        pattern = re.compile(r'```json-chart\s*(\{.*?\})\s*```', re.DOTALL)
        new_content = pattern.sub(replace_match, content)
        
        return new_content
